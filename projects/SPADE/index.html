<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>sparsity for interpretability | Arshia Soltani Moakhar</title> <meta name="author" content="Arshia Soltani Moakhar"/> <meta name="description" content="Leveraging sample sparsity to improve interpretability of neural networks"/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, research, Arshia Soltani Moakhar, ckodser, adversarial training, OOD detection"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon3.ico"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ckodser.github.io/projects/SPADE/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"><span class="font-weight-bold">Arshia </span>Soltani Moakhar</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/summaries/">Summaries</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">sparsity for interpretability</h1> <p class="post-description">Leveraging sample sparsity to improve interpretability of neural networks</p> </header> <article> <p>In this paper we improved interpretability methods by first pruning the model on the sample of interest and then apply the interpretability method. This approach let we apply global interpretability methods as a local interpretability. We showed with our experiments that this approach will improve the interpretability methods performance including feature visualization and saliency maps.</p> <h2> SPADE </h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/spade/images/SPADE_method_Square_jitter-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/spade/images/SPADE_method_Square_jitter-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/spade/images/SPADE_method_Square_jitter-1400.webp"></source> <img src="/assets/img/spade/images/SPADE_method_Square_jitter.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="SPADE" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> SPADE pipeline. </div> <p>As showed in the image, SPADE first uses augmentation to generate many similar images to the image of interest. It then prunes the network using these generated images. We used OBS and SparseGPT methods for pruning the network. Finally we apply the interpretability method to understand the network functioning on the sample of interest. In out experiments we applied Neuron Visualization techniques and Saliency Maps.</p> <h2> Why SPADE works? </h2> <p>As we know neural networks has many non-mono semantic neurons. These neurons get activated on many unrelated concepts. This property of neural networks makes it very hard to interpret their inner-working mechanism. When we apply SPADE we first prune the network on our specific sample. This means neurons focus on their functionality that is related to the sample and probably became mono-semantic. We show this in a toy example in the below figure.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/spade/images/all-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/spade/images/all-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/spade/images/all-1400.webp"></source> <img src="/assets/img/spade/images/all.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="SPADE on a toy model" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Two-dimensional example to illustrate the effect of SPADE on feature visualization. The feature visualizations are shown with green points, where blue and orange points are positive and negative samples. The SPADE Scenario 1 shows the feature visualizations obtained when the sample of interest is from larger positive region. Scenario 2 shows the visualizations obtained when the sample of interest is drawn from the smaller region. As we see although the final neuron is poly-semantic, when we prune the network using a sample, the neuron focus on the sample of interest and forgets its other functionality which makes interpretability easier. </div> <h2> SPADE Results</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/spade/images/SPADE_fig1_notrojan_nospadefirst-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/spade/images/SPADE_fig1_notrojan_nospadefirst-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/spade/images/SPADE_fig1_notrojan_nospadefirst-1400.webp"></source> <img src="/assets/img/spade/images/SPADE_fig1_notrojan_nospadefirst.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="SPADE on a toy model" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> (Left) When SPADE is applied with feature visualization it makes feature visualizations sample specific. Instead of answering what this neuron does in general we can now answer what this neuron does "in this sample". As you can see feature visualizations gives more information when SPADE is used. (Right) Image saliency maps are also have more detail. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/spade/images/backdoored_model-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/spade/images/backdoored_model-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/spade/images/backdoored_model-1400.webp"></source> <img src="/assets/img/spade/images/backdoored_model.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="SPADE on a toy model" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> We applied SPADE to a model with backdoor. The backdoor relates some emojy with some classes. When you apply feature visualization to those classes you could not see the neurons side functionality which is detecting the emoji. However with SPADE if you prune using an image with an emoji you could see the effect. In cases you prune using an image without emoji (clean image) you get a normal feature visualization. </div> <p>For code and experiment results visit <a href="https://github.com/IST-DASLab/SPADE/tree/main" target="_blank" rel="noopener noreferrer">GitHub</a>.</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Arshia Soltani Moakhar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>