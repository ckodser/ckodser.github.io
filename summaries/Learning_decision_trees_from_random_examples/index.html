<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning decision trees from random examples | Arshia Soltani Moakhar </title> <meta name="author" content="Arshia Soltani Moakhar"> <meta name="description" content="Decision tree learning By Finding Consistent Decision Trees"> <meta name="keywords" content="academic-website, research, Arshia Soltani Moakhar, ckodser, adversarial training, OOD detection, Interpretability"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon3.ico?ed0664bb8b662bf21b84ddd264d8c2a9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ckodser.github.io/summaries/Learning_decision_trees_from_random_examples/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class=" sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">Arshia</span> Soltani Moakhar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning decision trees from random examples</h1> <p class="post-description">Decision tree learning By Finding Consistent Decision Trees</p> <a href="https://www.sciencedirect.com/science/article/pii/0890540189900011" style="display: inline-block; padding: 8px 0; font-weight: 500; text-decoration: none; border-bottom: 2px solid transparent; transition: border-color 0.3s ease; background: linear-gradient(135deg, #5e72e4, #ff7eb3); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;" onmouseover="this.style.borderBottom='2px solid #ff7eb3'" onmouseout="this.style.borderBottom='2px solid transparent'" target="_blank" rel="noopener noreferrer">Read Paper →</a> </header> <article> <p>This summary focuses exclusively on the technical aspects of the paper concerning learning decision trees, including definitions, notations, the main theorems, and their proofs.</p> <h3 id="definitions-and-notations-for-decision-trees">Definitions and Notations for Decision Trees</h3> <ul> <li> <strong>Variables:</strong> \(V_n = {v_1, ..., v_n}\) is a set of \(n\) Boolean variables.</li> <li> <strong>Input Domain:</strong> \(X_n = {0,1}^n\).</li> <li> <strong>Class of Decision Trees (\(T_n\)):</strong> Recursively defined as follows: <ul> <li>A single root node labeled 0 or 1 is in \(T_n\) (abbreviated as \(Q=0\) or \(Q=1\)).</li> <li>If \(Q_0, Q_1 \in T_n\) and \(v \in V_n\), then the binary tree with root labeled \(v\), left subtree \(Q_0\) (0-subtree), and right subtree \(Q_1\) (1-subtree) is in \(T_n\).</li> </ul> </li> <li> <strong>Boolean Function Representation (\(f_Q\)):</strong> A decision tree \(Q \in T_n\) represents a Boolean function \(f_Q\): <ul> <li>If \(Q=0\), \(f_Q\) is the constant function 0.</li> <li>If \(Q=1\), \(f_Q\) is the constant function 1.</li> <li>Else if \(v_i\) is the label of the root of Q, \(Q_0\) the 0-subtree, and \(Q_1\) the 1-subtree, then for \(x=(a_1, ..., a_n) \in {0,1}^n\), if \(a_i=0\) then \(f_Q(x)=f_{Q_0}(x)\), else \(f_Q(x)=f_{Q_1}(x)\).</li> </ul> </li> <li> <strong>Reduced Decision Tree:</strong> A decision tree where each variable appears at most once in any path from the root to a leaf.</li> <li> <strong>Rank of a Decision Tree (\(r(Q)\)):</strong> Defined recursively: <ul> <li>If \(Q=0\) or \(Q=1\), then \(r(Q)=0\).</li> <li>Else if \(r_0\) is the rank of the 0-subtree of Q and \(r_1\) is the rank of the 1-subtree, then \(r(Q) = \max(r_0, r_1)\) if \(r_0 \ne r_1\), and \(r(Q) = r_0+1 (=r_1+1)\) otherwise.</li> </ul> </li> <li>\(T_n^r\): The set of all decision trees in \(T_n\) of rank at most \(r\).</li> <li>\(F_n^r\): The set of Boolean functions on \(X_n\) represented by trees in \(T_n^r\).</li> </ul> <hr> <h3 id="lemma-1">Lemma 1</h3> <ul> <li>Let \(k\) be the number of nodes in a reduced decision tree over \(V_n\) of rank \(r\), where \(n \ge r \ge 1\). Then \(2^{r+1}-1 \le k \le (2\sum_{i=0}^{r}\binom{n}{i})-1 &lt; 2(en/r)^r\).</li> <li> <table> <tbody> <tr> <td>If \(r=0\) then $$</td> <td>F_n^r</td> <td>=2\(. Else if\)n \le r\(then\)</td> <td>F_n^r</td> <td>=2^{2^n}\(, and if\)n &gt; r\(then\)</td> <td>F_n^r</td> <td>\le (8n)^{(en/r)^r}$$.</td> </tr> </tbody> </table> </li> </ul> <p><strong>Proof of Lemma 1:</strong></p> <p>(i) By induction, the smallest decision tree of rank \(r\) is a complete binary tree of depth \(r\), which has \(2^{r+1}-1\) nodes. Thus, \(2^{r+1}-1 \le k\). Let \(L(n,r)\) be the maximum number of leaves of any reduced decision tree over \(V_n\) of rank \(r\). From the definition of rank: \(L(0,r)=1\) for all \(r \ge 0\). \(L(n,0)=1\) for all \(n \ge 0\). \(L(n,r)=L(n-1,r)+L(n-1,r-1)\) for all \(n,r \ge 1\), because the variable in the root of a reduced tree does not appear in its subtrees. The solution for this recurrence for \(n \ge r\) is \(L(n,r)=\sum_{i=0}^{r}\binom{n}{i}\), which is bounded by \((en/r)^r\) for \(n \ge r \ge 1\). Since a binary tree has one less internal node than leaves, this yields the second and third inequalities for \(k\).</p> <p>(ii) If \(r=0\), \(F_n^r\) contains only constant functions, so \(|F_n^r|=2\). If \(n \le r\), \(T_n^r\) includes every full binary decision tree of depth \(n\), so \(F_n^r\) includes all Boolean functions on \(X_n\), and thus \(|F_n^r|=2^{2^n}\). If \(n &gt; r \ge 1\), each function in \(F_n^r\) is represented by a binary tree with at most \(k=(en/r)^r\) leaves. The number of distinct binary decision trees on \(n\) variables with at most \(k\) leaves is at most</p> <p>\(\sum_{i=1}^{k}\frac{2^{i}n^{i-1}}{2i-1}\binom{2i-1}{i}&lt;(2n)^{k}\sum_{i=1}^{k}\binom{2k-1}{i}&lt;(2n)^{k}2^{2k-1}&lt;(8n)^{k}\).</p> <table> <tbody> <tr> <td>Therefore, $$</td> <td>F_n^r</td> <td>\le (8n)^{(en/r)^r}$$.</td> </tr> </tbody> </table> <hr> <h3 id="definitions-for-finding-consistent-decision-trees">Definitions for Finding Consistent Decision Trees</h3> <ul> <li> <strong>Example:</strong> A pair \((x, f(x))\) for a Boolean function \(f\) on \(X_n\). It is positive if \(f(x)=1\), else negative.</li> <li> <strong>Rank of a Sample (\(r(S)\)):</strong> The minimum rank of any decision tree consistent with \(S\).</li> <li> <strong>Informative Variable:</strong> A variable \(v\) is informative on \(S\) if we have at least one sample with \(x_v=0\) and at least one with \(x_v=1\).</li> </ul> <h3 id="procedure-finds-r">Procedure FIND(S, r)</h3> <p><strong>Input:</strong> A nonempty sample \(S\) of some Boolean function on \(X_n\) and an integer \(r \ge 0\).</p> <p><strong>Output:</strong> A decision tree of rank at most \(r\) consistent with \(S\) if one exists, else “none”.</p> <ol> <li>If all examples in \(S\) are positive, return \(Q=1\); if all are negative, return \(Q=0\).</li> <li>If \(r=0\), return “none”.</li> <li>For each informative variable \(v \in V_n\): <ol> <li>Let \(Q_0^v = \text{FIND}(S_0^v, r-1)\) and \(Q_1^v = \text{FIND}(S_1^v, r-1)\).</li> <li>If both recursive calls succeed (neither \(Q_0^v=\text{none}\) nor \(Q_1^v=\text{none}\)), return the decision tree with root labeled \(v\), 0-subtree \(Q_0^v\), and 1-subtree \(Q_1^v\).</li> <li>If one recursive call succeeds but the other does not: <ol> <li>Reexecute the unsuccessful call with rank bound \(r\) instead of \(r-1\) (e.g., if \(Q_1^v\) is a tree but \(Q_0^v=\text{none}\), let \(Q_0^v = \text{FIND}(S_0^v, r)\)).</li> <li>If the reexecuted call succeeds, let \(Q\) be the decision tree with root labeled \(v\), 0-subtree \(Q_0^v\), and 1-subtree \(Q_1^v\), else let \(Q=\text{"none"}\).</li> <li>Return \(Q\).</li> </ol> </li> </ol> </li> <li>Return “none”.</li> </ol> <hr> <h3 id="lemma-3-time-of-find">Lemma 3 (Time of FIND)</h3> <table> <tbody> <tr> <td>For any nonempty sample \(S\) of a function on \(X_n\) and \(r \ge 0\), the time of FIND\((S,r)\) is $$O(</td> <td>S</td> <td>(n+1)^{2r})$$.</td> </tr> </tbody> </table> <p><strong>Proof of Lemma 3:</strong> Let \(T(i,r)\) be the max time for FIND\((S,r)\) when \(S\) is a sample on \(X_n\) with \(1 \le |S| \le m\) and at most \(i\) variables are informative. If \(i=0\), \(T(i,r)\) is \(O(1)\) (since \(|S|=1\)). If \(r=0\), \(T(i,r)\) is \(O(m)\). For \(r \ge 1\), steps 1 and 3 (determining informative variables) take \(O(mn)\) time. Each of the two recursive calls in step 3A takes at most \(T(i-1, r-1)\) time, as \(v\) is no longer informative. These calls are made at most \(i\) times, totaling \(2iT(i-1, r-1)\) for step 3A. Step 3C.1 makes at most one recursive call to \(FIND(S_0^v, r)\) or \(FIND(S_1^v, r)\), taking at most \(T(i-1, r)\) time. Therefore, for \(r \ge 1\): \(T(i,r) \le O(mn) + 2iT(i-1,r-1) + T(i-1,r)\). Given \(T(0,r) \le c_1\) and \(T(i,0) \le c_1\) for all \(i,r \ge 0\), and \(T(i,r) \le c_2 + 2iT(i-1,r-1) + T(i-1,r)\) for \(i,r \ge 1\), where \(c_1=O(m)\) and \(c_2=O(mn)\). It follows that \(T(i,r) \le c_2i + 2\sum_{j=1}^{i}jT(j-1,r-1)+c_1 \le c_1+c_2i+i(i+1)T(i,r-1)\). Solving this, \(T(i,r) &lt; c_1+c_2(i+1)+(i+1)^2T(i,r-1)\). This leads to \(T(i,r) &lt; c_2\sum_{j=0}^{r-1}(i+1)^{2j+1}+c_1\sum_{j=0}^{r}(i+1)^{2j} \le O(mn(i+1)^{2r-1}+m(i+1)^{2r})\). Since \(i \le n\) and \(m=|S|\), the time for FIND\((S,r)\) is \(O(|S|(n+1)^{2r})\).</p> <hr> <h3 id="theorem-1">Theorem 1</h3> <table> <tbody> <tr> <td>Given a sample \(S\) of a Boolean function on \(X_n\), using FINDMIN(S) (which iteratively calls FIND\((S,r)\) for \(r=0,1,2,...\) until a tree is returned), a decision tree consistent with \(S\) and having rank \(r(S)\) can be produced in time $$O(</td> <td>S</td> <td>(n+1)^{2r(S)})$$.</td> </tr> </tbody> </table> <h3 id="definition-of-error-in-learning">Definition of Error in Learning</h3> <ul> <li> <strong>Error of a hypothesis:</strong> For a probability distribution \(P\) on \(X_n\) and a target Boolean function \(f\) on \(X_n\), the error of a hypothesis \(g\) (w.r.t. \(f\) and \(P\)) is the probability that \(f(x) \ne g(x)\) for \(x\) drawn randomly from \(X_n\) according to \(P\).</li> </ul> <hr> <h3 id="lemma-4-blumer-et-al-1987">Lemma 4 (Blumer et al., 1987)</h3> <table> <tbody> <tr> <td>Let \(F_n\) be a class of Boolean functions on \(X_n\) and \(P\) be a probability distribution on \(X_n\). For any \(0 &lt; \epsilon, \delta &lt; 1\), and any target function \(f\) on \(X_n\), given a sequence of at least $$\frac{1}{\epsilon}\ln\frac{</td> <td>F_n</td> <td>}{\delta}\(random examples of\)f\((chosen independently according to\)P\(), with probability at least\)1-\delta\(, every hypothesis\)g \in F_n\(that is consistent with all of these examples has error at most\)\epsilon$$.</td> </tr> </tbody> </table> <p><strong>Proof of Lemma 4:</strong> For any single function with error at least \(\epsilon\), the probability that it is consistent with \(m\) random examples is at most \((1-\epsilon)^m \le e^{-\epsilon m}\). Hence, the probability that any function in \(F_n\) that has error at least \(\epsilon\) is consistent with \(m\) random examples is at most \(|F_n|e^{-\epsilon m}\). Setting this to \(\delta\) and solving for \(m\) gives the result.</p> <hr> <h3 id="theorem-2">Theorem 2</h3> <p>For any \(n \ge r \ge 1\), any target function \(f \in F_n^r\), any probability distribution \(P\) on \(X_n\), and any \(0 &lt; \epsilon, \delta &lt; 1\), given a sample \(S\) derived from a sequence of at least \(\frac{1}{\epsilon}((\frac{en}{r})^{r}\ln(8n)+\ln\frac{1}{\delta})\) random examples of \(f\) (chosen independently according to \(P\)), with probability at least \(1-\delta\), FIND\((S,r)\) (or FINDMIN(S)) produces a hypothesis \(g \in F_n^r\) that has error at most \(\epsilon\).</p> <p><strong>Proof of Theorem 2:</strong> By Lemma 1, \(|F_n^r| \le (8n)^{(en/r)^r}\) for \(n \ge r \ge 1\). Hence by Lemma 4, with probability at least \(1-\delta\), every hypothesis \(g \in F_n^r\) consistent with \(S\) has error at most \(\epsilon\). Since FIND\((S,r)\) and FINDMIN(S) produce one of these hypotheses, the result follows.</p> <hr> <h3 id="definition-of-f_ns">Definition of \(F_n^{(s)}\)</h3> <ul> <li>\(F_n^{(s)}\): The set of all Boolean functions on \(X_n\) represented by decision trees with at most \(s\) nodes.</li> </ul> <h3 id="lemma-5">Lemma 5</h3> <p>For all \(n, s \ge 1\), \(F_n^{(s)} \subseteq F_n^{\lfloor \log s \rfloor}\).</p> <p><strong>Proof of Lemma 5:</strong> In Lemma 1, it was shown that the smallest decision tree of rank \(r\) has at least \(2^{r+1}-1\) nodes. Thus, the rank of a decision tree with \(s\) nodes is at most \(\log(s+1)-1 \le \lfloor \log s \rfloor\).</p> <hr> <h3 id="theorem-3">Theorem 3</h3> <p>For any \(n, s \ge 1\), where \(n \ge \lfloor \log s \rfloor \ge 1\), any target function \(f \in F_n^{(s)}\), any probability distribution \(P\) on \(X_n\), and any \(0 &lt; \epsilon, \delta &lt; 1\), given a sample \(S\) derived from a sequence of at least \(\frac{1}{\epsilon}((\frac{en}{\lfloor \log s \rfloor})^{\lfloor \log s \rfloor}\log(8n)+\ln\frac{1}{\delta})\) random examples of \(f\) (chosen independently according to \(P\)), with probability at least \(1-\delta\), FINDMIN(S) produces a hypothesis \(g \in F_n^{\lfloor \log s \rfloor}\) that has error at most \(\epsilon\).</p> <p><strong>Proof of Theorem 3:</strong> This follows directly from Theorem 2 and Lemma 5.</p> <hr> <h3 id="corollary-1">Corollary 1</h3> <p>Let \(p(n)\) be any polynomial. There is a learning algorithm that, given random examples drawn according to any distribution on \({0,1}^n\) of any target function represented by a decision tree on \(n\) Boolean variables with at most \(p(n)\) nodes, produces, with probability at least \(1-\delta\), a hypothesis (represented as a decision tree) that has error at most \(\epsilon\). The number of random examples and computation time required is linear in \(n^{O(\log n)}\), \(1/\epsilon\), and \(\log(1/\delta)\).</p> <p><strong>Proof of Corollary 1:</strong> This follows directly from Theorems 1 and 3.</p> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ckodser/ckodser.github.io","data-repo-id":"R_kgDOHfm8sw","data-category":"General","data-category-id":"DIC_kwDOHfm8s84ClE4O","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"light","data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © 2025 Arshia Soltani Moakhar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"My Research Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Repositories to which I&#39;m a major contributor. Most of these works were done as a course final project.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-summaries",title:"Summaries",description:"Paper summaries",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"projects-data-leakage-of-lora-in-federated-training",title:"Data Leakage of LoRA in federated training",description:"This article highlights the potential risks of reconstructing private data from the gradients shared in Federated Learning, especially when using the LoRA finetuning technique.",section:"Projects",handler:()=>{window.location.href="/projects/Attention_is_all_you_need_to_attack/"}},{id:"projects-basedon",title:"BasedOn",description:"Using Learnable If Statements for Interpretability",section:"Projects",handler:()=>{window.location.href="/projects/BasedOn/"}},{id:"projects-sparsity-for-interpretability",title:"sparsity for interpretability",description:"Leveraging sample sparsity to improve interpretability of neural networks",section:"Projects",handler:()=>{window.location.href="/projects/SPADE/"}},{id:"projects-certified-robust-neural-network",title:"Certified Robust Neural Network",description:"Certify Robustness using median neurons",section:"Projects",handler:()=>{window.location.href="/projects/sparse_L_inf_network/"}},{id:"summaries-a-mathematical-framework-for-transformer-circuits",title:"A Mathematical Framework for Transformer Circuits",description:"In Transformers residual stream is the main object and layers read and write from/to it.",section:"Summaries",handler:()=>{window.location.href="/summaries/A_Mathematical_Framework_for_Transformer_Circuits/"}},{id:"summaries-an-overview-of-early-vision-in-inceptionv1",title:"An Overview of Early Vision in InceptionV1",description:"inceptionV1 feature maps of different layers",section:"Summaries",handler:()=>{window.location.href="/summaries/An_Overview_of_Early_Vision_in_InceptionV1/"}},{id:"summaries-clip-dissect-automatic-description-of-neuron-representations",title:"CLIP-Dissect Automatic Description of Neuron Representations",description:"Find concepts that activates a neuron using a image dataset",section:"Summaries",handler:()=>{window.location.href="/summaries/CLIP-Dissect_Automatic_Description_of_Neuron_Representations_in_Deep_Vision_Networks/"}},{id:"summaries-can-large-language-models-explain-their-internal-mechanisms",title:"Can Large Language Models Explain Their Internal Mechanisms?",description:"summary of Can Large Language Models Explain Their Internal Mechanisms?",section:"Summaries",handler:()=>{window.location.href="/summaries/Can_Large_Language_Models_Explain_Their_Internal_Mechanisms/"}},{id:"summaries-emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task",title:"Emergent World Representations Exploring a Sequence Model Trained on a Synthetic Task",description:"summary of Emergent World Representations  Exploring a Sequence Model Trained on a Synthetic Task",section:"Summaries",handler:()=>{window.location.href="/summaries/Emergent_World_Representations_Exploring_a_Sequence_Model_Trained_on_a_Synthetic_Task/"}},{id:"summaries-every-decision-tree-has-an-influential-variable",title:"Every decision tree has an influential variable",description:"title is self-explanatory",section:"Summaries",handler:()=>{window.location.href="/summaries/Every_decision_tree_has_an_influential_variable/"}},{id:"summaries-interpretability-beyond-feature-attribution-quantitative-testing-with-concept-activation-vectors-tcav",title:"Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV)",description:"summary of Interpretability Beyond Feature Attribution  Quantitative Testing with Concept Activation Vectors (TCAV)",section:"Summaries",handler:()=>{window.location.href="/summaries/Interpretability_Beyond_Feature_Attribution_Quantitative_Testing_with_Concept_Activation_Vectors_(TCAV)/"}},{id:"summaries-labeling-neural-representations-with-inverse-recognition",title:"Labeling Neural Representations with Inverse Recognition",description:"summary of Labeling Neural Representations with Inverse Recognition",section:"Summaries",handler:()=>{window.location.href="/summaries/Labeling_Neural_Representations_with_Inverse_Recognition/"}},{id:"summaries-learning-decision-trees-from-random-examples",title:"Learning decision trees from random examples",description:"Decision tree learning By Finding Consistent Decision Trees",section:"Summaries",handler:()=>{window.location.href="/summaries/Learning_decision_trees_from_random_examples/"}},{id:"summaries-leveraged-volume-sampling-for-linear-regression",title:"Leveraged volume sampling for linear regression",description:"Active Learning in linear regression with multiplicative error rate bounds",section:"Summaries",handler:()=>{window.location.href="/summaries/Leveraged_volume_sampling_for_linear_regression/"}},{id:"summaries-progress-measures-for-grokking-via-mechanistic-interpretability",title:"Progress measures for grokking via mechanistic interpretability",description:"summary of Progress measures for grokking via mechanistic interpretability",section:"Summaries",handler:()=>{window.location.href="/summaries/Progress_measures_for_grokking_via_mechanistic_interpretability/"}},{id:"summaries-properly-learning-decision-trees-in-almost-polynomial-time",title:"Properly learning decision trees in almost polynomial time",description:"learning a decision tree for unifrom random data distribution in O(s ^ log(log(s)))",section:"Summaries",handler:()=>{window.location.href="/summaries/Properly_learning-_decision_trees_in_almost_polynomial_time/"}},{id:"summaries-scaling-monosemanticity-extracting-interpretable-features-from-claude-3-sonnet",title:"Scaling Monosemanticity Extracting Interpretable Features from Claude 3 Sonnet",description:"Scale SAE to Claude 3 Sonnet",section:"Summaries",handler:()=>{window.location.href="/summaries/Scaling_Monosemanticity_Extracting_Interpretable_Features_from_Claude_3_Sonnet/"}},{id:"summaries-top-down-induction-of-decision-trees-rigorous-guarantees-and-inherent-limitations",title:"Top-down induction of decision trees- rigorous guarantees and inherent limitations",description:"greedily learn a decision tree based on the most inflouential variables in all leaves.",section:"Summaries",handler:()=>{window.location.href="/summaries/Top_down_induction_of_decision_trees_rigorous_guarantees_and_inherent_limitations/"}},{id:"summaries-towards-monosemanticity-decomposing-language-models-with-dictionary-learning",title:"Towards Monosemanticity Decomposing Language Models With Dictionary Learning",description:"How SAE works",section:"Summaries",handler:()=>{window.location.href="/summaries/Towards_Monosemanticity_Decomposing_Language_Models_With_Dictionary_Learning/"}},{id:"summaries-what-do-we-learn-from-inverting-clip-models",title:"What do we learn from inverting CLIP models?",description:"summary of What do we learn from inverting CLIP models?",section:"Summaries",handler:()=>{window.location.href="/summaries/What_do_we_learn_from_inverting_CLIP_models/"}},{id:"summaries-zoom-in-an-introduction-to-circuits",title:"Zoom In An Introduction to Circuits",description:"Investigate Vision Circuits by Studying the Connections between Neurons",section:"Summaries",handler:()=>{window.location.href="/summaries/Zoom_In_An_Introduction_to_Circuits/"}},{id:"summaries-active-learning-survey",title:"Active Learning Survey",description:"Active Learning for Agnostic classification",section:"Summaries",handler:()=>{window.location.href="/summaries/active-survey/"}},{id:"summaries-the-true-sample-complexity-of-active-learning",title:"The True Sample Complexity of Active Learning",description:"A different definition of active learning label complexity",section:"Summaries",handler:()=>{window.location.href="/summaries/true-active/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%72%73%68%69%61.%73%6F%6C%74%61%6E%69%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ckodser","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>