<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning decision trees from random examples | Arshia Soltani Moakhar </title> <meta name="author" content="Arshia Soltani Moakhar"> <meta name="description" content="Decision tree learning By Finding Consistent Decision Trees"> <meta name="keywords" content="academic-website, research, Arshia Soltani Moakhar, ckodser, adversarial training, OOD detection, Interpretability"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon3.ico?ed0664bb8b662bf21b84ddd264d8c2a9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ckodser.github.io/summaries/Learning_decision_trees_from_random_examples/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class=" sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">Arshia</span> Soltani Moakhar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning decision trees from random examples</h1> <p class="post-description">Decision tree learning By Finding Consistent Decision Trees</p> <a href="https://www.sciencedirect.com/science/article/pii/0890540189900011" style="display: inline-block; padding: 8px 0; font-weight: 500; text-decoration: none; border-bottom: 2px solid transparent; transition: border-color 0.3s ease; background: linear-gradient(135deg, #5e72e4, #ff7eb3); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;" onmouseover="this.style.borderBottom='2px solid #ff7eb3'" onmouseout="this.style.borderBottom='2px solid transparent'" target="_blank" rel="noopener noreferrer">Read Paper →</a> </header> <article> <p>They first show that if they have a realizable function, and they sample some datapoints, then a consistent decision tree with these samples has a low error. Then They proposed an algorithm that finds a decision tree with min rank that is consistent with the samples. Then they move their analysis from rank to nodes.</p> <h3 id="definitions-and-notations-for-decision-trees">Definitions and Notations for Decision Trees</h3> <ul> <li> <strong>Variables:</strong> \(V_n = {v_1, ..., v_n}\) is a set of \(n\) Boolean variables.</li> <li> <strong>Input Domain:</strong> \(X_n = {0,1}^n\).</li> <li> <strong>Class of Decision Trees (\(T_n\)):</strong> Recursively defined as follows: <ul> <li>A single root node labeled 0 or 1 is in \(T_n\) (abbreviated as \(Q=0\) or \(Q=1\)).</li> <li>If \(Q_0, Q_1 \in T_n\) and \(v \in V_n\), then the binary tree with root labeled \(v\), left subtree \(Q_0\) (0-subtree), and right subtree \(Q_1\) (1-subtree) is in \(T_n\).</li> </ul> </li> <li> <strong>Boolean Function Representation (\(f_Q\)):</strong> A decision tree \(Q \in T_n\) represents a Boolean function \(f_Q\): <ul> <li>If \(Q=0\), \(f_Q\) is the constant function 0.</li> <li>If \(Q=1\), \(f_Q\) is the constant function 1.</li> <li>Else if \(v_i\) is the label of the root of Q, \(Q_0\) the 0-subtree, and \(Q_1\) the 1-subtree, then for \(x=(a_1, ..., a_n) \in {0,1}^n\), if \(a_i=0\) then \(f_Q(x)=f_{Q_0}(x)\), else \(f_Q(x)=f_{Q_1}(x)\).</li> </ul> </li> <li> <strong>Reduced Decision Tree:</strong> A decision tree where each variable appears at most once in any path from the root to a leaf.</li> <li> <strong>Rank of a Decision Tree (\(r(Q)\)):</strong> Defined recursively: <ul> <li>If \(Q=0\) or \(Q=1\), then \(r(Q)=0\).</li> <li>Else if \(r_0\) is the rank of the 0-subtree of Q and \(r_1\) is the rank of the 1-subtree, then \(r(Q) = \max(r_0, r_1)\) if \(r_0 \ne r_1\), and \(r(Q) = r_0+1 (=r_1+1)\) otherwise.</li> </ul> </li> <li>\(T_n^r\): The set of all decision trees in \(T_n\) of rank at most \(r\).</li> <li>\(F_n^r\): The set of Boolean functions on \(X_n\) represented by trees in \(T_n^r\).</li> <li> <strong>Rank of a Sample (\(r(S)\))</strong>: The minimum rank of any decision tree consistent with \(S\).</li> <li> <strong>Informative Variable</strong>: A variable \(v\) is informative on \(S\) if we have at least one sample with \(x_v=0\) and at least one with \(x_v=1\).</li> <li> <strong>Error of a hypothesis:</strong> For a probability distribution \(P\) on \(X_n\) and a target Boolean function \(f\) on \(X_n\), the error of a hypothesis \(g\) (w.r.t. \(f\) and \(P\)) is the probability that \(f(x) \ne g(x)\) for \(x\) drawn randomly from \(X_n\) according to \(P\).</li> <li>\(F_n^{(s)}\): The set of all Boolean functions on \(X_n\) represented by decision trees with at most \(s\) nodes.</li> </ul> <hr> <h2 id="procedure-finds-r">Procedure FIND(S, r)</h2> <p><strong>Input:</strong> A nonempty sample \(S\) of some Boolean function on \(X_n\) and an integer \(r \geq 0\).</p> <p><strong>Output:</strong> A decision tree of rank at most \(r\) consistent with \(S\) if one exists, else “none”.</p> <ol> <li>If all examples in \(S\) are positive, return \(Q=1\); if all are negative, return \(Q=0\).</li> <li>If \(r=0\), return “none”.</li> <li>For each informative variable \(v \in V_n\): <ol> <li>Let \(Q_0^v = \text{FIND}(S_0^v, r-1)\) and \(Q_1^v = \text{FIND}(S_1^v, r-1)\).</li> <li>If both recursive calls succeed (neither \(Q_0^v=\text{none}\) nor \(Q_1^v=\text{none}\)), return the decision tree with root labeled \(v\), 0-subtree \(Q_0^v\), and 1-subtree \(Q_1^v\).</li> <li>If one recursive call succeeds but the other does not: <ol> <li>Reexecute the unsuccessful call with rank bound \(r\) instead of \(r-1\) (e.g., if \(Q_1^v\) is a tree but \(Q_0^v=\text{none}\), let \(Q_0^v = \text{FIND}(S_0^v, r)\)).</li> <li>If the reexecuted call succeeds, let \(Q\) be the decision tree with root labeled \(v\), 0-subtree \(Q_0^v\), and 1-subtree \(Q_1^v\), else let \(Q=\text{"none"}\).</li> <li>Return \(Q\).</li> </ol> </li> </ol> </li> <li>Return “none”.</li> </ol> <hr> <h2 id="lemma-2-correctness-of-find">Lemma 2 (Correctness of FIND)</h2> <p>The procedure FIND is correct.</p> <h3 id="proof-of-lemma-2"><strong>Proof of Lemma 2:</strong></h3> <p>Let \(m=|S|\). Correctness is by induction on \(m\) and \(r\).</p> <p><strong>Base</strong>: If \(m=1\) or \(r=0\), FIND\((S,r)\) is easily verified to be correct.</p> <p><strong>Step</strong>:Assume \(S\) has \(|S|=m \geq 2\) and \(r \geq 1\). Assume the procedure is correct for \(r-1\) with arbitrary size \(S\), and for \(r\) when \(S\) has size less than \(m\). Since \(|S_0^v| &lt; |S|\) and \(|S_1^v| &lt; |S|\) for any informative variable \(v\), if FIND\((S,r)\) returns a tree (in step 1, 3.2, or 3.3), by the inductive hypothesis and definition of rank, it will be a tree of rank at most \(r\) consistent with \(S\). If “none” is returned and \(r \ge 1\), execution stops in step 3.3 or 4. If in 3.3, by induction, either \(r(S_0^v) &gt; r\) or \(r(S_1^v) &gt; r\) for some \(v\), implying \(r(S) &gt; r\). If in step 4, by induction, \(r(S_0^v) \geq r\) and \(r(S_1^v) \geq r\) for every informative variable \(v\). As execution didn’t halt at step 1, any decision tree consistent with \(S\) must have a variable at its root.</p> <hr> <h2 id="lemma-3-time-of-find">Lemma 3 (Time of FIND)</h2> <p>For any nonempty sample \(S\) of a function on \(X_n\) and \(r \geq 0\), the time of FIND\((S,r)\) is \(O(|S|(n+1)^{2r})\).</p> <h3 id="proof-of-lemma-3"><strong>Proof of Lemma 3:</strong></h3> <p>Let \(T(i,r)\) be the max time for FIND\((S,r)\) when \(S\) is a sample on \(X_n\) with \(1 \leq |S| \leq m\) and at most \(i\) variables are informative. If \(i=0\), \(T(i,r)\) is \(O(1)\) (since \(|S|=1\)). If \(r=0\), \(T(i,r)\) is \(O(m)\).</p> <p><strong>determining informative variables</strong>: For \(r \geq 1\), steps 1 and 3 () take \(O(mn)\) time.</p> <p><strong>step 3.1</strong>: Each of the two recursive calls in step 3.1 takes at most \(T(i-1, r-1)\) time, as \(v\) is no longer informative. These calls are made at most \(i\) times, totaling \(2iT(i-1, r-1)\) for step 3.1.</p> <p><strong>Step 3.3.1</strong>: Step 3.3.1 makes at most one recursive call to \(FIND(S_0^v, r)\) or \(FIND(S_1^v, r)\), taking at most \(T(i-1, r)\) time.</p> <p>Therefore, for \(r \geq 1\): \(T(i,r) \leq O(mn) + 2iT(i-1,r-1) + T(i-1,r)\). Given \(T(0,r) \leq c_1\)</p> <p>and \(T(i,0) \leq c_1\) for all \(i,r \geq 0\), and \(T(i,r) \leq c_2 + 2iT(i-1,r-1) + T(i-1,r)\) for \(i,r \geq 1\), where \(c_1=O(m)\) and \(c_2=O(mn)\).</p> <p>It follows that \(T(i,r) \leq c_2i + 2\sum_{j=1}^{i}jT(j-1,r-1)+c_1 \leq c_1+c_2i+i(i+1)T(i,r-1)\).</p> <p>Solving this, \(T(i,r) &lt; c_1+c_2(i+1)+(i+1)^2T(i,r-1)\).</p> <p>This leads to \(T(i,r) &lt; c_2\sum_{j=0}^{r-1}(i+1)^{2j+1}+c_1\sum_{j=0}^{r}(i+1)^{2j} \leq O(mn(i+1)^{2r-1}+m(i+1)^{2r})\).</p> <table> <tbody> <tr> <td>Since \(i \leq n\) and $$m=</td> <td>S</td> <td>$$,</td> </tr> <tr> <td>the time for FIND\((S,r)\) is $$O(</td> <td>S</td> <td>(n+1)^{2r}).$$</td> </tr> </tbody> </table> <hr> <h2 id="theorem-1">Theorem 1</h2> <p>Given a sample \(S\) of a Boolean function on \(X_n\), using FINDMIN\((S)\) (which iteratively calls FIND\((S,r)\) for \(r=0,1,2,...\) until a tree is returned), a decision tree consistent with \(S\) and having rank \(r(S)\) can be produced in time \(O(|S|(n+1)^{2r(S)})\).</p> <hr> <h2 id="lemma-4-blumer-et-al-1987">Lemma 4 (Blumer et al., 1987)</h2> <p>Let \(F_n\) be a class of Boolean functions on \(X_n\) and \(P\) be a probability distribution on \(X_n\). For any \(0 &lt; \epsilon, \delta &lt; 1\), and any target function \(f\) on \(X_n\), given a sequence of at least \(\frac{1}{\epsilon}\ln\frac{|F_n|}{\delta}\) random examples of \(f\) (chosen independently according to \(P\)), with probability at least \(1-\delta\), every hypothesis \(g \in F_n\) that is consistent with all of these examples has error at most \(\epsilon\).</p> <h3 id="proof-of-lemma-4"><strong>Proof of Lemma 4:</strong></h3> <p>For any single function with error at least \(\epsilon\), the probability that it is consistent with \(m\) random examples is at most \((1-\epsilon)^m \leq e^{-\epsilon m}\). Hence, the probability that any function in \(F_n\) that has error at least \(\epsilon\) is consistent with \(m\) random examples is at most \(|F_n|e^{-\epsilon m}\). Setting this to \(\delta\) and solving for \(m\) gives the result.</p> <hr> <h2 id="lemma-1">Lemma 1</h2> <p>(i) Let \(k\) be the number of nodes in a reduced decision tree over \(V_n\) of rank \(r\), where \(n \geq r \geq 1\). Then \(2^{r+1}-1 \leq k \leq (2\sum_{i=0}^{r}\binom{n}{i})-1 &lt; 2(\frac{en}{r})^r\).</p> <p>(ii) If \(r=0\) then \(|F_n^r|=2\). Else if \(n\leq r\) then \(|F_n^r|=2^{2^n}\), and if \(n&gt;r\) then \(|F_n^r|\leq (8n)^{(\frac{en}{r})^r}\).</p> <h3 id="proof-of-lemma-1"><strong>Proof of Lemma 1:</strong></h3> <p>(i) By induction, the smallest decision tree of rank \(r\) is a complete binary tree of depth \(r\), which has \(2^{r+1}-1\) nodes. Thus, \(2^{r+1}-1 \leq k\). Let \(L(n,r)\) be the maximum number of leaves of any reduced decision tree over \(V_n\) of rank \(r\). From the definition of rank: \(L(0,r)=1\) for all \(r \geq 0\). \(L(n,0)=1\) for all \(n \geq 0\). \(L(n,r)=L(n-1,r)+L(n-1,r-1)\) for all \(n,r \geq 1\), because the variable in the root of a reduced tree does not appear in its subtrees. The solution for this recurrence for \(n \geq r\) is \(L(n,r)=\sum_{i=0}^{r}\binom{n}{i}\), which is bounded by \((\frac{en}{r})^r\) for \(n \geq r \geq 1\). Since a binary tree has one less internal node than leaves, this yields the second and third inequalities for \(k\).</p> <p>(ii) If \(r=0\), \(F_n^r\) contains only constant functions, so \(|F_n^r|=2\). If \(n \leq r\), \(T_n^r\) includes every full binary decision tree of depth \(n\), so \(F_n^r\) includes all Boolean functions on \(X_n\), and thus \(|F_n^r|=2^{2^n}\). If \(n &gt; r \geq 1\), each function in \(F_n^r\) is represented by a binary tree with at most \(k=(\frac{en}{r})^r\) leaves. The number of distinct binary decision trees on \(n\) variables with at most \(k\) leaves is at most</p> <p>\(\sum_{i=1}^{k}\frac{2^{i}n^{i-1}}{2i-1}\binom{2i-1}{i}&lt;(2n)^{k}\sum_{i=1}^{k}\binom{2k-1}{i}&lt;(2n)^{k}2^{2k-1}&lt;(8n)^{k}\).</p> <p>Therefore,</p> \[|F_n^r| \leq (8n)^{(\frac{en}{r})^r}.\] <hr> <h2 id="theorem-2">Theorem 2</h2> <p>For any \(n \geq r \geq 1\), any target function \(f \in F_n^r\), any probability distribution \(P\) on \(X_n\), and any \(0 &lt; \epsilon, \delta &lt; 1\), given a sample \(S\) derived from a sequence of at least \(\frac{1}{\epsilon}((\frac{en}{r})^{r}\ln(8n)+\ln\frac{1}{\delta})\) random examples of \(f\) (chosen independently according to \(P\)), with probability at least \(1-\delta\), FIND\((S,r)\) (or FINDMIN\((S)\)) produces a hypothesis \(g \in F_n^r\) that has error at most \(\epsilon\).</p> <h3 id="proof-of-theorem-2"><strong>Proof of Theorem 2:</strong></h3> <p>By Lemma 1, \(|F_n^r| \leq (8n)^{(\frac{en}{r})^r}\) for \(n \geq r \geq 1\). Hence by Lemma 4, with probability at least \(1-\delta\), every hypothesis \(g \in F_n^r\) consistent with \(S\) has error at most \(\epsilon\). Since FIND\((S,r)\) and FINDMIN\((S)\) produce one of these hypotheses, the result follows.</p> <hr> <h2 id="lemma-5">Lemma 5</h2> <p>For all \(n, s \geq 1\), \(F_n^{(s)} \subseteq F_n^{\lfloor \log s \rfloor}\).</p> <h3 id="proof-of-lemma-5"><strong>Proof of Lemma 5:</strong></h3> <p>In Lemma 1, it was shown that the smallest decision tree of rank \(r\) has at least \(2^{r+1}-1\) nodes. Thus, the rank of a decision tree with \(s\) nodes is at most \(\log(s+1)-1 \leq \lfloor \log s \rfloor\).</p> <hr> <h2 id="theorem-3">Theorem 3</h2> <p>For any \(n, s \geq 1\), where \(n \geq \lfloor \log s \rfloor \geq 1\), any target function \(f \in F_n^{(s)}\), any probability distribution \(P\) on \(X_n\), and any \(0 &lt; \epsilon, \delta &lt; 1\), given a sample \(S\) derived from a sequence of at least \(\frac{1}{\epsilon}((\frac{en}{\lfloor \log s \rfloor})^{\lfloor \log s \rfloor}\log(8n)+\ln\frac{1}{\delta})\) random examples of \(f\) (chosen independently according to \(P\)), with probability at least \(1-\delta\), FINDMIN\((S)\) produces a hypothesis \(g \in F_n^{\lfloor \log s \rfloor}\) that has error at most \(\epsilon\).</p> <h3 id="proof-of-theorem-3"><strong>Proof of Theorem 3:</strong></h3> <p>This follows directly from Theorem 2 and Lemma 5.</p> <hr> <h2 id="corollary-1">Corollary 1</h2> <p>Let \(p(n)\) be any polynomial. There is a learning algorithm that, given random examples drawn according to any distribution on \({0,1}^n\) of any target function represented by a decision tree on \(n\) Boolean variables with at most \(p(n)\) nodes, produces, with probability at least \(1-\delta\), a hypothesis (represented as a decision tree) that has error at most \(\epsilon\). The number of random examples and computation time required is linear in \(n^{O(\log n)}\), \(1/\epsilon\), and \(\log(1/\delta)\).</p> <h3 id="proof-of-corollary-1"><strong>Proof of Corollary 1:</strong></h3> <p>This follows directly from Theorems 1 and 3.</p> <hr> <p>Some notes on Corollary 1 (a simpler form)</p> <h2 id="corollary-2">Corollary 2</h2> <p>There is a learning algorithm that, given random examples drawn according to any distribution on \({0,1}^n\) of any target function represented by a decision tree on \(n\) Boolean variables with at most \(p\in \text{polynomial}(n)\) nodes, produces, with probability at least \(1-\delta\), a hypothesis (represented as a decision tree) that has error at most \(\epsilon\), and rank at most \(\lfloor \log p \rfloor\in O(\text{polylog}(n))\). The number of random examples required is</p> \[O(\frac{1}{\epsilon}\cdot(\log(1/\delta)+\log(n)\cdot(\frac{en}{\lfloor \log p \rfloor})^{\lfloor \log p \rfloor})) &lt; O(\frac{1}{\epsilon}\cdot(\log(1/\delta)+\log(n)\cdot(\frac{en})^{\text{polylog(n)})) =?= O(\frac{1}{\epsilon}\cdot(\log(1/\delta)+\cdot(\frac{en})^{\text{polylog(n)}))\] <p>and computation time required is</p> <p>\(O(\frac{1}{\epsilon}\cdot(\log(1/\delta)+\cdot(\frac{en})^{\text{polylog(n)})\codt n^{2\log(p)})&lt; O(\frac{1}{\epsilon}\cdot(\log(1/\delta)+\cdot(\frac{en})^{\text{polylog(n)})) n^{2\text{polylog}(p)})= O(\frac{1}{\epsilon}\cdot(\log(1/\delta)+\cdot(\frac{en})^{\text{polylog(n)}))\).</p> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ckodser/ckodser.github.io","data-repo-id":"R_kgDOHfm8sw","data-category":"General","data-category-id":"DIC_kwDOHfm8s84ClE4O","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"light","data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © 2026 Arshia Soltani Moakhar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"My Research Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Repositories to which I&#39;m a major contributor. Most of these works were done as a course final project.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-summaries",title:"Summaries",description:"Paper summaries",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"projects-data-leakage-of-lora-in-federated-training",title:"Data Leakage of LoRA in federated training",description:"This article highlights the potential risks of reconstructing private data from the gradients shared in Federated Learning, especially when using the LoRA finetuning technique.",section:"Projects",handler:()=>{window.location.href="/projects/Attention_is_all_you_need_to_attack/"}},{id:"projects-basedon",title:"BasedOn",description:"Using Learnable If Statements for Interpretability",section:"Projects",handler:()=>{window.location.href="/projects/BasedOn/"}},{id:"projects-sparsity-for-interpretability",title:"sparsity for interpretability",description:"Leveraging sample sparsity to improve interpretability of neural networks",section:"Projects",handler:()=>{window.location.href="/projects/SPADE/"}},{id:"projects-certified-robust-neural-network",title:"Certified Robust Neural Network",description:"Certify Robustness using median neurons",section:"Projects",handler:()=>{window.location.href="/projects/sparse_L_inf_network/"}},{id:"summaries-a-mathematical-framework-for-transformer-circuits",title:"A Mathematical Framework for Transformer Circuits",description:"In Transformers residual stream is the main object and layers read and write from/to it.",section:"Summaries",handler:()=>{window.location.href="/summaries/A_Mathematical_Framework_for_Transformer_Circuits/"}},{id:"summaries-an-overview-of-early-vision-in-inceptionv1",title:"An Overview of Early Vision in InceptionV1",description:"inceptionV1 feature maps of different layers",section:"Summaries",handler:()=>{window.location.href="/summaries/An_Overview_of_Early_Vision_in_InceptionV1/"}},{id:"summaries-clip-dissect-automatic-description-of-neuron-representations",title:"CLIP-Dissect Automatic Description of Neuron Representations",description:"Find concepts that activates a neuron using a image dataset",section:"Summaries",handler:()=>{window.location.href="/summaries/CLIP-Dissect_Automatic_Description_of_Neuron_Representations_in_Deep_Vision_Networks/"}},{id:"summaries-can-large-language-models-explain-their-internal-mechanisms",title:"Can Large Language Models Explain Their Internal Mechanisms?",description:"summary of Can Large Language Models Explain Their Internal Mechanisms?",section:"Summaries",handler:()=>{window.location.href="/summaries/Can_Large_Language_Models_Explain_Their_Internal_Mechanisms/"}},{id:"summaries-dash-detection-and-assessment-of-systematic-hallucinations-of-vlms",title:"DASH Detection and Assessment of Systematic Hallucinations of VLMs",description:"Make a dataset that VLMs hallucinate and wrongly think things exist in images",section:"Summaries",handler:()=>{window.location.href="/summaries/DASH_Detection_and_Assessment_of_Systematic_Hallucinations_of_VLMs/"}},{id:"summaries-emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task",title:"Emergent World Representations Exploring a Sequence Model Trained on a Synthetic Task",description:"summary of Emergent World Representations  Exploring a Sequence Model Trained on a Synthetic Task",section:"Summaries",handler:()=>{window.location.href="/summaries/Emergent_World_Representations_Exploring_a_Sequence_Model_Trained_on_a_Synthetic_Task/"}},{id:"summaries-every-decision-tree-has-an-influential-variable",title:"Every decision tree has an influential variable",description:"title is self-explanatory",section:"Summaries",handler:()=>{window.location.href="/summaries/Every_decision_tree_has_an_influential_variable/"}},{id:"summaries-interpretability-beyond-feature-attribution-quantitative-testing-with-concept-activation-vectors-tcav",title:"Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV)",description:"summary of Interpretability Beyond Feature Attribution  Quantitative Testing with Concept Activation Vectors (TCAV)",section:"Summaries",handler:()=>{window.location.href="/summaries/Interpretability_Beyond_Feature_Attribution_Quantitative_Testing_with_Concept_Activation_Vectors_(TCAV)/"}},{id:"summaries-labeling-neural-representations-with-inverse-recognition",title:"Labeling Neural Representations with Inverse Recognition",description:"summary of Labeling Neural Representations with Inverse Recognition",section:"Summaries",handler:()=>{window.location.href="/summaries/Labeling_Neural_Representations_with_Inverse_Recognition/"}},{id:"summaries-learning-decision-trees-from-random-examples",title:"Learning decision trees from random examples",description:"Decision tree learning By Finding Consistent Decision Trees",section:"Summaries",handler:()=>{window.location.href="/summaries/Learning_decision_trees_from_random_examples/"}},{id:"summaries-leveraged-volume-sampling-for-linear-regression",title:"Leveraged volume sampling for linear regression",description:"Active Learning in linear regression with multiplicative error rate bounds",section:"Summaries",handler:()=>{window.location.href="/summaries/Leveraged_volume_sampling_for_linear_regression/"}},{id:"summaries-physics-of-language-models",title:"Physics of Language Models",description:"Understanding LLMs by training smaller LMs in controlled environment",section:"Summaries",handler:()=>{window.location.href="/summaries/Physics_of_LM/"}},{id:"summaries-progress-measures-for-grokking-via-mechanistic-interpretability",title:"Progress measures for grokking via mechanistic interpretability",description:"summary of Progress measures for grokking via mechanistic interpretability",section:"Summaries",handler:()=>{window.location.href="/summaries/Progress_measures_for_grokking_via_mechanistic_interpretability/"}},{id:"summaries-properly-learning-decision-trees-in-almost-polynomial-time",title:"Properly learning decision trees in almost polynomial time",description:"learning a decision tree for unifrom random data distribution in O(s ^ log(log(s)))",section:"Summaries",handler:()=>{window.location.href="/summaries/Properly_learning-_decision_trees_in_almost_polynomial_time/"}},{id:"summaries-scaling-monosemanticity-extracting-interpretable-features-from-claude-3-sonnet",title:"Scaling Monosemanticity Extracting Interpretable Features from Claude 3 Sonnet",description:"Scale SAE to Claude 3 Sonnet",section:"Summaries",handler:()=>{window.location.href="/summaries/Scaling_Monosemanticity_Extracting_Interpretable_Features_from_Claude_3_Sonnet/"}},{id:"summaries-top-down-induction-of-decision-trees-rigorous-guarantees-and-inherent-limitations",title:"Top-down induction of decision trees- rigorous guarantees and inherent limitations",description:"greedily learn a decision tree based on the most inflouential variables in all leaves.",section:"Summaries",handler:()=>{window.location.href="/summaries/Top_down_induction_of_decision_trees_rigorous_guarantees_and_inherent_limitations/"}},{id:"summaries-towards-monosemanticity-decomposing-language-models-with-dictionary-learning",title:"Towards Monosemanticity Decomposing Language Models With Dictionary Learning",description:"How SAE works",section:"Summaries",handler:()=>{window.location.href="/summaries/Towards_Monosemanticity_Decomposing_Language_Models_With_Dictionary_Learning/"}},{id:"summaries-what-do-we-learn-from-inverting-clip-models",title:"What do we learn from inverting CLIP models?",description:"summary of What do we learn from inverting CLIP models?",section:"Summaries",handler:()=>{window.location.href="/summaries/What_do_we_learn_from_inverting_CLIP_models/"}},{id:"summaries-zoom-in-an-introduction-to-circuits",title:"Zoom In An Introduction to Circuits",description:"Investigate Vision Circuits by Studying the Connections between Neurons",section:"Summaries",handler:()=>{window.location.href="/summaries/Zoom_In_An_Introduction_to_Circuits/"}},{id:"summaries-active-learning-survey",title:"Active Learning Survey",description:"Active Learning for Agnostic classification",section:"Summaries",handler:()=>{window.location.href="/summaries/active-survey/"}},{id:"summaries-the-true-sample-complexity-of-active-learning",title:"The True Sample Complexity of Active Learning",description:"A different definition of active learning label complexity",section:"Summaries",handler:()=>{window.location.href="/summaries/true-active/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%72%73%68%69%61.%73%6F%6C%74%61%6E%69%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=2H6Wl4MAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ckodser","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>