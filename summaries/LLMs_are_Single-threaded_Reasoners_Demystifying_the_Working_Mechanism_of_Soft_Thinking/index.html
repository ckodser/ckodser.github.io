<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LLMs are Single-threaded Reasoners, Demystifying the Working Mechanism of Soft Thinking | Arshia Soltani Moakhar </title> <meta name="author" content="Arshia Soltani Moakhar"> <meta name="description" content="Vanilla Soft Thinking pushes the model to the greedy token sampling internally. They showed that the model usually continues to work only with the most probable next token. To mitigate this issue they suggest adding noise to logits and get better performance."> <meta name="keywords" content="academic-website, research, Arshia Soltani Moakhar, ckodser, adversarial training, OOD detection, Interpretability"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?16404ec2cd2689e8d0f38f73fe0d38f9"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon3.ico?ed0664bb8b662bf21b84ddd264d8c2a9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ckodser.github.io/summaries/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class=" sticky-bottom-footer collection-summaries"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">Arshia</span> Soltani Moakhar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">LLMs are Single-threaded Reasoners, Demystifying the Working Mechanism of Soft Thinking</h1> <p class="post-description">Vanilla Soft Thinking pushes the model to the greedy token sampling internally. They showed that the model usually continues to work only with the most probable next token. To mitigate this issue they suggest adding noise to logits and get better performance.</p> <a href="https://arxiv.org/pdf/2508.03440" style="display: inline-block; padding: 8px 0; font-weight: 500; text-decoration: none; border-bottom: 2px solid transparent; transition: border-color 0.3s ease; background: linear-gradient(135deg, #5e72e4, #ff7eb3); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;" onmouseover="this.style.borderBottom='2px solid #ff7eb3'" onmouseout="this.style.borderBottom='2px solid transparent'" target="_blank" rel="noopener noreferrer">Read Paper →</a> </header> <article> <p>They first use soft thinking in some benchmarks and show that it gets worse performance compared to normal thinking.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/image4.png" sizes="95vw"></source> <img src="/assets/img/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/image4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>To investigate this, they propose the hypothesis that the model takes the most likely token and continues with that. If this is true, it means soft thinking is just greedy decoding.</p> <blockquote> <p>Hypothesis: LLMs are Single-Threaded Reasoners</p> <p>LLMs lack the ability to process multiple different semantic trajectories in parallel. When a Soft Token is fed into an LLM, the generation process is typically dominated by the majority component of the Soft Token.</p> </blockquote> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/image3.png" sizes="95vw"></source> <img src="/assets/img/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/image3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>To test this, they replace a soft token with a discrete token, either the 1st dominant token in the soft thinking or the 2nd dominant token in the soft thinking. Then they measure the divergence in the predicted probabilities. For instance, assume we have question \(X\) and a soft thinking reasoning of \([S_1, S_2, \cdots, S_n]\). We investigate the i-th token, so we replace it with 1st dominant token \(T_i^1\) or second dominant token \(T_i^2\). We measure the JS Divergence of \(S_{i+1}\) with the model prediction when given: \([X, S_1, S_2, \cdots, S_{i-1}, T_i^1]\) and \([X, S_1, S_2, \cdots, S_{i-1}, T_i^2]\). They showed that for most cases the JS Divergence between \(S_{i+1}\) and replacement of \(T_i^1\) is very low and very high with replacement of \(T_i^2\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/image5.png" sizes="95vw"></source> <img src="/assets/img/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/image5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Then they try to figure out where the model ignores other tokens and starts to only consider the most dominant token. They find some soft thinking reasoning tokens that have two different dominant tokens, \(T_i^1\) and \(T_i^2\) for instance. Then they make a soft thinking token consist only of those two tokens: \(S_i=0.6 T_i^1 + 0.4 T_i^2\). They again test</p> <p>1) \([X, S_1, S_2, \cdots, S_{i-1}, T_i^1]\)</p> <p>2) \([X, S_1, S_2, \cdots, S_{i-1}, T_i^2]\)</p> <p>3) \([X, S_1, S_2, \cdots, S_{i-1}, S_i]\)</p> <p>They get the top 5 predicted tokens for the first two lines. Then they investigate the behaviour of the soft thinking (third line). They see that when the model goes deeper into layers, the predicted tokens have less and less intersection with the second dominant token prediction and more intersection with the first dominant token prediction.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/image6.png" sizes="95vw"></source> <img src="/assets/img/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/image6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><em>They ignored the fact that the second dominant token continues to have 0.2 intersection until the end. Is it possible that the soft thinking top 10 guesses consist of 9 from the most dominant token and 1 from the less dominant token?</em></p> <p>Finally, they also take a soft thinking reasoning path and extract the most dominant token at each position to form an explicit reasoning path. They see that it is more aligned with greedy decoding (taking the token with the highest probability at each step) than with normal decoding (sampling tokens randomly based on model logits).</p> <h2 id="methods">Methods</h2> <p>They investigate two types of noise they can add to logits:</p> <ul> <li> <p>Dirichlet sampling: First multiply probabilities by a constant \(0 &lt; \gamma &lt; &lt; 1\). Then get the probabilities from \(Dir(\gamma p)\). The Dirichlet sampling works as follows: sample \(Y_i\sim gamma(\alpha_i, \text{scale}=1)\). Then \(\hat{\pi}_i= \frac{Y_i}{\sum_j Y_j}\).</p> </li> <li> <p>Gumbel-Softmax Trick: Assume we have probabilities \(\pi_i\). We want to make it noisy \(\hat{\pi}_i\). We sample a noise for each element \(g_i\sim Gumbel(0,1)\), and then</p> </li> </ul> \[\hat{\pi}_i = \frac{\exp((g_i + \log(\pi_i))/\tau)}{ \sum_k^n \exp((g_k + \log(\pi_k))/\tau)}\] <p>They showed that Gumbel works very well and performs better than Dirichlet.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/image7.png" sizes="95vw"></source> <img src="/assets/img/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/image7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="grpo">GRPO</h1> <p>In GRPO we need a policy ratio for each token:</p> \[r_t(\theta) = \frac{\pi_{\theta}(y_t \mid x, y_{&lt;t})}{\pi_{\text{ref}}(y_t \mid x, y_{&lt;t})}\] <p>In soft thinking, we don’t have \(y_t\) as there is no selected token. They propose we can do this instead: define the probability of choosing that soft-thinking token as</p> \[p_{\pi, \tau}(y_1, \dots, y_n) = \Gamma(n) \tau^{n-1} \left( \sum_{i=1}^n \frac{\pi_i}{y_i^\tau} \right) \prod_{i=1}^n \frac{\pi_i}{y_i^{\tau+1}}\] <p>and then for soft-thinking \(i\) we have: \(r_t(\theta) = \frac{p_{\pi_{\theta}, \tau}(y_1, \dots, y_n)}{p_{\pi_{\text{ref}}, \tau}(y_1, \dots, y_n)}\).</p> <p>They did not do GRPO themselves.</p> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ckodser/ckodser.github.io","data-repo-id":"R_kgDOHfm8sw","data-category":"General","data-category-id":"DIC_kwDOHfm8s84ClE4O","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"light","data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © 2026 Arshia Soltani Moakhar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/summary_image_expand.js?de7a891220c8fc9ce98add9ca26b742e"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?f8abf2f636f242d077f24149a0a56c96"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"My Research Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-summaries",title:"Summaries",description:"Paper summaries",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"projects-data-leakage-of-lora-in-federated-training",title:"Data Leakage of LoRA in federated training",description:"This article highlights the potential risks of reconstructing private data from the gradients shared in Federated Learning, especially when using the LoRA finetuning technique.",section:"Projects",handler:()=>{window.location.href="/projects/Attention_is_all_you_need_to_attack/"}},{id:"projects-basedon",title:"BasedOn",description:"Using Learnable If Statements for Interpretability",section:"Projects",handler:()=>{window.location.href="/projects/BasedOn/"}},{id:"projects-sparsity-for-interpretability",title:"sparsity for interpretability",description:"Leveraging sample sparsity to improve interpretability of neural networks",section:"Projects",handler:()=>{window.location.href="/projects/SPADE/"}},{id:"projects-certified-robust-neural-network",title:"Certified Robust Neural Network",description:"Certify Robustness using median neurons",section:"Projects",handler:()=>{window.location.href="/projects/sparse_L_inf_network/"}},{id:"summaries-a-mathematical-framework-for-transformer-circuits",title:"A Mathematical Framework for Transformer Circuits",description:"In Transformers residual stream is the main object and layers read and write from/to it.",section:"Summaries",handler:()=>{window.location.href="/summaries/A_Mathematical_Framework_for_Transformer_Circuits/"}},{id:"summaries-an-overview-of-early-vision-in-inceptionv1",title:"An Overview of Early Vision in InceptionV1",description:"inceptionV1 feature maps of different layers",section:"Summaries",handler:()=>{window.location.href="/summaries/An_Overview_of_Early_Vision_in_InceptionV1/"}},{id:"summaries-clip-dissect-automatic-description-of-neuron-representations",title:"CLIP-Dissect Automatic Description of Neuron Representations",description:"Find concepts that activates a neuron using a image dataset",section:"Summaries",handler:()=>{window.location.href="/summaries/CLIP-Dissect_Automatic_Description_of_Neuron_Representations_in_Deep_Vision_Networks/"}},{id:"summaries-can-large-language-models-explain-their-internal-mechanisms",title:"Can Large Language Models Explain Their Internal Mechanisms?",description:"summary of Can Large Language Models Explain Their Internal Mechanisms?",section:"Summaries",handler:()=>{window.location.href="/summaries/Can_Large_Language_Models_Explain_Their_Internal_Mechanisms/"}},{id:"summaries-dash-detection-and-assessment-of-systematic-hallucinations-of-vlms",title:"DASH Detection and Assessment of Systematic Hallucinations of VLMs",description:"Make a dataset that VLMs hallucinate and wrongly think things exist in images",section:"Summaries",handler:()=>{window.location.href="/summaries/DASH_Detection_and_Assessment_of_Systematic_Hallucinations_of_VLMs/"}},{id:"summaries-deep-learning-is-not-so-mysterious-or-different",title:"Deep Learning is Not So Mysterious or Different",description:"??",section:"Summaries",handler:()=>{window.location.href="/summaries/Deep_Learning_is_Not_So_Mysterious_or_Different/"}},{id:"summaries-emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task",title:"Emergent World Representations Exploring a Sequence Model Trained on a Synthetic Task",description:"summary of Emergent World Representations  Exploring a Sequence Model Trained on a Synthetic Task",section:"Summaries",handler:()=>{window.location.href="/summaries/Emergent_World_Representations_Exploring_a_Sequence_Model_Trained_on_a_Synthetic_Task/"}},{id:"summaries-every-decision-tree-has-an-influential-variable",title:"Every decision tree has an influential variable",description:"title is self-explanatory",section:"Summaries",handler:()=>{window.location.href="/summaries/Every_decision_tree_has_an_influential_variable/"}},{id:"summaries-interpretability-beyond-feature-attribution-quantitative-testing-with-concept-activation-vectors-tcav",title:"Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV)",description:"summary of Interpretability Beyond Feature Attribution  Quantitative Testing with Concept Activation Vectors (TCAV)",section:"Summaries",handler:()=>{window.location.href="/summaries/Interpretability_Beyond_Feature_Attribution_Quantitative_Testing_with_Concept_Activation_Vectors_(TCAV)/"}},{id:"summaries-llm-latent-reasoning-as-chain-of-superposition",title:"LLM Latent Reasoning as Chain of Superposition",description:"Train an encoder that summarizes reasoning chunks. Then train a latent reasoning model on the summaries it produces from some CoT data.",section:"Summaries",handler:()=>{window.location.href="/summaries/LLM_Latent_Reasoning_as_Chain_of_Superposition/"}},{id:"summaries-llms-are-single-threaded-reasoners-demystifying-the-working-mechanism-of-soft-thinking",title:"LLMs are Single-threaded Reasoners, Demystifying the Working Mechanism of Soft Thinking",description:"Vanilla Soft Thinking pushes the model to the greedy token sampling internally. They showed that the model usually continues to work only with the most probable next token. To mitigate this issue they suggest adding noise to logits and get better performance.",section:"Summaries",handler:()=>{window.location.href="/summaries/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/"}},{id:"summaries-labeling-neural-representations-with-inverse-recognition",title:"Labeling Neural Representations with Inverse Recognition",description:"summary of Labeling Neural Representations with Inverse Recognition",section:"Summaries",handler:()=>{window.location.href="/summaries/Labeling_Neural_Representations_with_Inverse_Recognition/"}},{id:"summaries-learning-decision-trees-from-random-examples",title:"Learning decision trees from random examples",description:"Decision tree learning By Finding Consistent Decision Trees",section:"Summaries",handler:()=>{window.location.href="/summaries/Learning_decision_trees_from_random_examples/"}},{id:"summaries-leveraged-volume-sampling-for-linear-regression",title:"Leveraged volume sampling for linear regression",description:"Active Learning in linear regression with multiplicative error rate bounds",section:"Summaries",handler:()=>{window.location.href="/summaries/Leveraged_volume_sampling_for_linear_regression/"}},{id:"summaries-multiplex-thinking-reasoning-via-token-wise-branch-and-merge",title:"Multiplex Thinking Reasoning via Token wise Branch and Merge",description:"Make soft-thinking a bit random. Then train with GRPO.",section:"Summaries",handler:()=>{window.location.href="/summaries/Multiplex_Thinking_Reasoning_via_Token_wise_Branch_and_Merge/"}},{id:"summaries-physics-of-language-models",title:"Physics of Language Models",description:"Understanding LLMs by training smaller LMs in controlled environment",section:"Summaries",handler:()=>{window.location.href="/summaries/Physics_of_LM/"}},{id:"summaries-progress-measures-for-grokking-via-mechanistic-interpretability",title:"Progress measures for grokking via mechanistic interpretability",description:"summary of Progress measures for grokking via mechanistic interpretability",section:"Summaries",handler:()=>{window.location.href="/summaries/Progress_measures_for_grokking_via_mechanistic_interpretability/"}},{id:"summaries-properly-learning-decision-trees-in-almost-polynomial-time",title:"Properly learning decision trees in almost polynomial time",description:"learning a decision tree for unifrom random data distribution in O(s ^ log(log(s)))",section:"Summaries",handler:()=>{window.location.href="/summaries/Properly_learning-_decision_trees_in_almost_polynomial_time/"}},{id:"summaries-reasoning-within-the-mind-dynamic-multimodal-interleaving-in-latent-space",title:"Reasoning Within the Mind Dynamic Multimodal Interleaving in Latent Space",description:"Training free latent reasoning. Optimize the latent reasoning tokens to maximize the model confidence which is correlated with the model accuracy.",section:"Summaries",handler:()=>{window.location.href="/summaries/Reasoning_Within_the_Mind_Dynamic_Multimodal_Interleaving_in_Latent_Space/"}},{id:"summaries-scaling-monosemanticity-extracting-interpretable-features-from-claude-3-sonnet",title:"Scaling Monosemanticity Extracting Interpretable Features from Claude 3 Sonnet",description:"Scale SAE to Claude 3 Sonnet",section:"Summaries",handler:()=>{window.location.href="/summaries/Scaling_Monosemanticity_Extracting_Interpretable_Features_from_Claude_3_Sonnet/"}},{id:"summaries-soft-tokens-hard-truths",title:"Soft Tokens, Hard Truths",description:"They add Gaussian noise to the soft-thinking embeddings, then train with RL using RLOO.",section:"Summaries",handler:()=>{window.location.href="/summaries/Soft_Tokens_Hard_Truths/"}},{id:"summaries-top-down-induction-of-decision-trees-rigorous-guarantees-and-inherent-limitations",title:"Top-down induction of decision trees- rigorous guarantees and inherent limitations",description:"greedily learn a decision tree based on the most inflouential variables in all leaves.",section:"Summaries",handler:()=>{window.location.href="/summaries/Top_down_induction_of_decision_trees_rigorous_guarantees_and_inherent_limitations/"}},{id:"summaries-towards-monosemanticity-decomposing-language-models-with-dictionary-learning",title:"Towards Monosemanticity Decomposing Language Models With Dictionary Learning",description:"How SAE works",section:"Summaries",handler:()=>{window.location.href="/summaries/Towards_Monosemanticity_Decomposing_Language_Models_With_Dictionary_Learning/"}},{id:"summaries-what-do-we-learn-from-inverting-clip-models",title:"What do we learn from inverting CLIP models?",description:"summary of What do we learn from inverting CLIP models?",section:"Summaries",handler:()=>{window.location.href="/summaries/What_do_we_learn_from_inverting_CLIP_models/"}},{id:"summaries-zoom-in-an-introduction-to-circuits",title:"Zoom In An Introduction to Circuits",description:"Investigate Vision Circuits by Studying the Connections between Neurons",section:"Summaries",handler:()=>{window.location.href="/summaries/Zoom_In_An_Introduction_to_Circuits/"}},{id:"summaries-active-learning-survey",title:"Active Learning Survey",description:"Active Learning for Agnostic classification",section:"Summaries",handler:()=>{window.location.href="/summaries/active-survey/"}},{id:"summaries-the-true-sample-complexity-of-active-learning",title:"The True Sample Complexity of Active Learning",description:"A different definition of active learning label complexity",section:"Summaries",handler:()=>{window.location.href="/summaries/true-active/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%72%73%68%69%61.%73%6F%6C%74%61%6E%69%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=2H6Wl4MAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ckodser","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>