<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Every decision tree has an influential variable | Arshia Soltani Moakhar </title> <meta name="author" content="Arshia Soltani Moakhar"> <meta name="description" content="title is self-explanatory"> <meta name="keywords" content="academic-website, research, Arshia Soltani Moakhar, ckodser, adversarial training, OOD detection, Interpretability"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon3.ico?ed0664bb8b662bf21b84ddd264d8c2a9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ckodser.github.io/summaries/Every_decision_tree_has_an_influential_variable/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class=" sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">Arshia</span> Soltani Moakhar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Every decision tree has an influential variable</h1> <p class="post-description">title is self-explanatory</p> <a href="https://arxiv.org/abs/cs/0508071" style="display: inline-block; padding: 8px 0; font-weight: 500; text-decoration: none; border-bottom: 2px solid transparent; transition: border-color 0.3s ease; background: linear-gradient(135deg, #5e72e4, #ff7eb3); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;" onmouseover="this.style.borderBottom='2px solid #ff7eb3'" onmouseout="this.style.borderBottom='2px solid transparent'" target="_blank" rel="noopener noreferrer">Read Paper</a> </header> <article> <p>This summary focuses exclusively on the technical aspects of the paper concerning boolean functions \(f:\{-1,1\}^{n}\rightarrow\{-1,1\}\), including definitions, notations, the main theorem, and its proof as they apply to this specific case.</p> <h3 id="definitions-and-notations">Definitions and Notations</h3> <ul> <li> <strong>Input Domain:</strong> The domain is \(\{-1,1\}^{n}_{(p)}\), which represents \(n\) binary input variables. The measure is taken with respect to an arbitrary product measure \(p\) on \(\{-1,1\}^{n}\). When written simply \(\{-1,1\}^{n}\), the uniform measure case where \(p=1/2\) is implied.</li> <li> <strong>Function Output:</strong> The output is binary, \(f:\{-1,1\}^{n}\rightarrow\{-1,1\}\).</li> <li> <strong>Variance (\(Var[f]\)):</strong> For a function \(f:\{-1,1\}_{(p)}^{n}\rightarrow\{-1,1\}\), the variance is defined as \(Var[f] = E[f^2] - E[f]^2 = 4 \Pr[f=1]\Pr[f=-1]\). This measures the “balance” of the function.</li> <li> <strong>Influence (\(Inf_{i}(f)\)):</strong> The influence of the \(i\)-th coordinate on \(f\) is defined as \(Inf_{i}(f) = 2 \Pr_{x,x^{(i)}}[f(x) \neq f(x^{(i)})]\). Here, \(x\) is drawn from \(\{-1,1\}_{(p)}^{n}\) and \(x^{(i)}\) is formed by rerandomizing the \(i\)-th coordinate of \(x\) using the probability measure \(p_i\). In the uniform measure case (\(p=1/2\)), this definition aligns with \(Inf_{i}[f] = \Pr[f(x) \ne f(x \oplus i)]\).</li> <li> <strong>DDT:</strong> A decision tree.</li> <li> <strong>Probability of Querying a Variable (\(\delta_{i}(T)\)):</strong> For a DDT \(T\) computing a function \(f:\{-1,1\}_{(p)}^{n}\rightarrow\{-1,1\}\), \(\delta_{i}(T) = \Pr_{x\in\{-1,1\}_{(p)}^{n}}[T \text{ queries } x_{i}]\).</li> <li> <strong>Expected Cost (\(\Delta(T)\)):</strong> For a DDT \(T\), the expected cost is \(\Delta(T) = \sum_{i=1}^{n} \delta_{i}(T) = E[\text{#coords T queries on } x]\).</li> <li> <strong>Minimum Expected Cost (\(\Delta(f)\)):</strong> \(\Delta(f)\) denotes the minimum of \(\Delta(T)\) over all DDTs \(T\) computing \(f:\{-1,1\}_{(p)}^{n}\rightarrow\{-1,1\}\).</li> </ul> <h3 id="main-theorems-and-proofs">Main Theorems and Proofs</h3> <p><strong>Theorem 1.1:</strong> Let \(f:\{-1,1\}_{(p)}^{n}\rightarrow\{-1,1\}\) and let \(T\) be a DDT computing \(f\). Then</p> \[Var[f]\le\sum_{i=1}^{n}\delta_{i}(T)Inf_{i}(f).\] <p><strong>Proof of Theorem 1.1:</strong> Let \(x\) and \(y\) be random inputs chosen independently from \(\Omega = (\{-1,1\}^n, \mu_{(p)})\). For a subset \(J\subseteq[n]\), let \(x_{J}y\) denote the hybrid input that agrees with \(x\) on coordinates in \(J\) and with \(y\) on coordinates in \([n]\setminus J\).</p> <p>Let \(i_1, \dots, i_s\) be the sequence of variables queried by \(T\) on input \(x\). \(s\) is a random variable representing the number of queried variables. For \(t \ge 0\), let \(J[t] = \{i_r : s \ge r &gt; t\}\). It is the dimensions of the last \(s-t\) node in the root to \(x\) path.</p> <p>last Define \(u[t] = x_{J[t]}y\). \(u[t]\) means the datapoint agreeing with \(x\) on the last \(s-t\) dimensions’ of the nodes in the root to \(x\) path and other dimensions are set to match that of \(y\).</p> <p><strong>We are slowly transforming \(x\) to \(y\) by slowly changing its dimension one by one to match that of \(y\). We are doing it in a way that if in a step the label changed, that is counted in influence of some node. This is why we start from the bottom and go up. This way when a dimension changes if the labels also change, that is counted in influence of that node.</strong></p> <p>We start with the observation that \(Var[f] = E[f(x) \neq f(y)] = E[f(u[0]) \neq f(u[s])]\). This follows because \(y = u[s]\) and \(f(x) = f(u[0])\) as \(T\) computes \(f\).</p> \[E[f(u[0])\neq f(u[s])] \le E\left[\sum_{t=1}^{s} f(u[t-1])\neq f(u[t])\right]\] <p><strong>The above inequality only means if \(f(x)\neq f(y)\) the label should have changed somewhere in the process.</strong> Notice that \(u[t-1]\) and \(u[t]\) differ only on one dimension assume \(i(t)\). Splitting the expeted value based on \(i(t)\), we get:</p> \[Var[f] \leq \sum_{t=1}^{n} \sum_{i=1}^{n} E[(f(u[t-1])\neq f(u[t]))\cdot 1_{\{i_t=i\}}]\] <p>We define \(X_t\) as the sequence of variables that \(T\) queries on \(x\) till time \(t\). \(X_t = (x_{i_1}, x_{i_2}, \cdots, x_{i_{\min(t, s)}})\)</p> \[Var[f] \leq \sum_{t=1}^{n} \sum_{i=1}^{n} E[E[(f(u[t-1])\neq f(u[t]))\cdot 1_{\{i_t=i\}}|X_{t-1}]]\] <p>Obviously, given the random variable \(X_{t-1}\) we can determine the first \(t\) node in the path thus determining \(i_t\). Proof magic happened here: we can show that conditional on \(X_{t-1}\), the pair \((u[t-1], u[t])\) has the distribution \(\Omega^{(i_t)}\) if \(i_t = i \in [n]\).</p> <blockquote> <p>This is because \(u[t-1]\) and \(u[t]\) differ only along a single dimension, which is \(i(t)\). Additionally, \(y\) is unrelated to \(X_{t-1}\), so its distribution is independent. In \(u[t] = x_{J[t]} y\), the dimensions not contained in \(J[t]\) are set to match those of \(y\); therefore, they retain their original distribution \(\Omega\). The dimensions in \(J[t]\) are set to those of \(x\), specifically the \(t, t+1, \ldots, s\)-th dimensions along the root-to-\(x\) path. These dimensions are distinct from the first \(t-1\) dimensions of \(x\) and thus also preserve their original distribution. As a result, \(u[t]\) is distributed according to \(\Omega^{(i_t)}\) whenever \(i_t = i \in [n]\). This means,</p> <table> <tbody> <tr> <td>$$E[(f(u[t-1])\neq f(u[t]))\cdot 1_{{i_t=i}}</td> <td>X_{t-1}]=E_{x, y \sim \Omega^{(i_t)}}[(f(x)\neq f(y))\cdot 1<em>{{i_t(x)=i}}]=Inf_i(f) 1</em>{i_t(x)=i}$$.</td> </tr> </tbody> </table> </blockquote> <p>Using the above</p> \[Var[f] \leq \sum_{t=1}^{n} \sum_{i=1}^{n} E[Inf_i(f)\cdot 1_{\{i_t=i\}}] = \sum_{i=1}^{n} Inf_i(f)\sum_{t=1}^{n} E[1_{\{i_t=i\}}] = \sum_{i=1}^{n} Inf_i(f) \delta_i(T)\] <hr> <p><strong>Corollary 1.2:</strong> For every \(f:\{-1,1\}_{(p)}^{n}\rightarrow\{-1,1\}\), we have</p> \[\Delta(f)\ge\frac{Var(f)}{Inf_{max}(f)}\] <p>where \(Inf_{max}(f) = \max\{Inf_i(f) : i \in [n]\}\).</p> <p><strong>Proof of Corollary 1.2:</strong> Let \(T\) be a DDT computing \(f\). From Theorem 1.1:</p> \[Var[f]\le\sum_{i=1}^{n}\delta_{i}(T)Inf_{i}(f)\] \[Var[f]\le Inf_{max}(f)\sum_{i=1}^{n}\delta_{i}(T) = Inf_{max}(f)\cdot\Delta(T)\] <p>Since this holds for any DDT \(T\) computing \(f\), it holds for the one minimizing \(\Delta(T)\), which is \(\Delta(f)\).</p> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ckodser/ckodser.github.io","data-repo-id":"R_kgDOHfm8sw","data-category":"General","data-category-id":"DIC_kwDOHfm8s84ClE4O","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"light","data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © 2025 Arshia Soltani Moakhar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"My Research Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Repositories to which I&#39;m a major contributor. Most of these works were done as a course final project.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-summaries",title:"Summaries",description:"Paper summaries",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"projects-data-leakage-of-lora-in-federated-training",title:"Data Leakage of LoRA in federated training",description:"This article highlights the potential risks of reconstructing private data from the gradients shared in Federated Learning, especially when using the LoRA finetuning technique.",section:"Projects",handler:()=>{window.location.href="/projects/Attention_is_all_you_need_to_attack/"}},{id:"projects-basedon",title:"BasedOn",description:"Using Learnable If Statements for Interpretability",section:"Projects",handler:()=>{window.location.href="/projects/BasedOn/"}},{id:"projects-sparsity-for-interpretability",title:"sparsity for interpretability",description:"Leveraging sample sparsity to improve interpretability of neural networks",section:"Projects",handler:()=>{window.location.href="/projects/SPADE/"}},{id:"projects-certified-robust-neural-network",title:"Certified Robust Neural Network",description:"Certify Robustness using median neurons",section:"Projects",handler:()=>{window.location.href="/projects/sparse_L_inf_network/"}},{id:"summaries-a-mathematical-framework-for-transformer-circuits",title:"A Mathematical Framework for Transformer Circuits",description:"In Transformers residual stream is the main object and layers read and write from/to it.",section:"Summaries",handler:()=>{window.location.href="/summaries/A_Mathematical_Framework_for_Transformer_Circuits/"}},{id:"summaries-an-overview-of-early-vision-in-inceptionv1",title:"An Overview of Early Vision in InceptionV1",description:"inceptionV1 feature maps of different layers",section:"Summaries",handler:()=>{window.location.href="/summaries/An_Overview_of_Early_Vision_in_InceptionV1/"}},{id:"summaries-clip-dissect-automatic-description-of-neuron-representations",title:"CLIP-Dissect Automatic Description of Neuron Representations",description:"Find concepts that activates a neuron using a image dataset",section:"Summaries",handler:()=>{window.location.href="/summaries/CLIP-Dissect_Automatic_Description_of_Neuron_Representations_in_Deep_Vision_Networks/"}},{id:"summaries-can-large-language-models-explain-their-internal-mechanisms",title:"Can Large Language Models Explain Their Internal Mechanisms?",description:"summary of Can Large Language Models Explain Their Internal Mechanisms?",section:"Summaries",handler:()=>{window.location.href="/summaries/Can_Large_Language_Models_Explain_Their_Internal_Mechanisms/"}},{id:"summaries-emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task",title:"Emergent World Representations Exploring a Sequence Model Trained on a Synthetic Task",description:"summary of Emergent World Representations  Exploring a Sequence Model Trained on a Synthetic Task",section:"Summaries",handler:()=>{window.location.href="/summaries/Emergent_World_Representations_Exploring_a_Sequence_Model_Trained_on_a_Synthetic_Task/"}},{id:"summaries-every-decision-tree-has-an-influential-variable",title:"Every decision tree has an influential variable",description:"title is self-explanatory",section:"Summaries",handler:()=>{window.location.href="/summaries/Every_decision_tree_has_an_influential_variable/"}},{id:"summaries-interpretability-beyond-feature-attribution-quantitative-testing-with-concept-activation-vectors-tcav",title:"Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV)",description:"summary of Interpretability Beyond Feature Attribution  Quantitative Testing with Concept Activation Vectors (TCAV)",section:"Summaries",handler:()=>{window.location.href="/summaries/Interpretability_Beyond_Feature_Attribution_Quantitative_Testing_with_Concept_Activation_Vectors_(TCAV)/"}},{id:"summaries-labeling-neural-representations-with-inverse-recognition",title:"Labeling Neural Representations with Inverse Recognition",description:"summary of Labeling Neural Representations with Inverse Recognition",section:"Summaries",handler:()=>{window.location.href="/summaries/Labeling_Neural_Representations_with_Inverse_Recognition/"}},{id:"summaries-leveraged-volume-sampling-for-linear-regression",title:"Leveraged volume sampling for linear regression",description:"Active Learning in linear regression with multiplicative error rate bounds",section:"Summaries",handler:()=>{window.location.href="/summaries/Leveraged_volume_sampling_for_linear_regression/"}},{id:"summaries-progress-measures-for-grokking-via-mechanistic-interpretability",title:"Progress measures for grokking via mechanistic interpretability",description:"summary of Progress measures for grokking via mechanistic interpretability",section:"Summaries",handler:()=>{window.location.href="/summaries/Progress_measures_for_grokking_via_mechanistic_interpretability/"}},{id:"summaries-properly-learning-decision-trees-in-almost-polynomial-time",title:"Properly learning decision trees in almost polynomial time",description:"learning a decision tree for unifrom random data distribution in O(s ^ log(log(s)))",section:"Summaries",handler:()=>{window.location.href="/summaries/Properly_learning-_decision_trees_in_almost_polynomial_time/"}},{id:"summaries-scaling-monosemanticity-extracting-interpretable-features-from-claude-3-sonnet",title:"Scaling Monosemanticity Extracting Interpretable Features from Claude 3 Sonnet",description:"Scale SAE to Claude 3 Sonnet",section:"Summaries",handler:()=>{window.location.href="/summaries/Scaling_Monosemanticity_Extracting_Interpretable_Features_from_Claude_3_Sonnet/"}},{id:"summaries-top-down-induction-of-decision-trees-rigorous-guarantees-and-inherent-limitations",title:"Top-down induction of decision trees- rigorous guarantees and inherent limitations",description:"greedily learn a decision tree based on the most inflouential variables in all leaves.",section:"Summaries",handler:()=>{window.location.href="/summaries/Top_down_induction_of_decision_trees_rigorous_guarantees_and_inherent_limitations/"}},{id:"summaries-towards-monosemanticity-decomposing-language-models-with-dictionary-learning",title:"Towards Monosemanticity Decomposing Language Models With Dictionary Learning",description:"How SAE works",section:"Summaries",handler:()=>{window.location.href="/summaries/Towards_Monosemanticity_Decomposing_Language_Models_With_Dictionary_Learning/"}},{id:"summaries-what-do-we-learn-from-inverting-clip-models",title:"What do we learn from inverting CLIP models?",description:"summary of What do we learn from inverting CLIP models?",section:"Summaries",handler:()=>{window.location.href="/summaries/What_do_we_learn_from_inverting_CLIP_models/"}},{id:"summaries-zoom-in-an-introduction-to-circuits",title:"Zoom In An Introduction to Circuits",description:"Investigate Vision Circuits by Studying the Connections between Neurons",section:"Summaries",handler:()=>{window.location.href="/summaries/Zoom_In_An_Introduction_to_Circuits/"}},{id:"summaries-active-learning-survey",title:"Active Learning Survey",description:"Active Learning for Agnostic classification",section:"Summaries",handler:()=>{window.location.href="/summaries/active-survey/"}},{id:"summaries-the-true-sample-complexity-of-active-learning",title:"The True Sample Complexity of Active Learning",description:"A different definition of active learning label complexity",section:"Summaries",handler:()=>{window.location.href="/summaries/true-active/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%72%73%68%69%61.%73%6F%6C%74%61%6E%69%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ckodser","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>