<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Top-down induction of decision trees- rigorous guarantees and inherent limitations | Arshia Soltani Moakhar </title> <meta name="author" content="Arshia Soltani Moakhar"> <meta name="description" content="greedily learn a decision tree based on the most inflouential variables in all leaves."> <meta name="keywords" content="academic-website, research, Arshia Soltani Moakhar, ckodser, adversarial training, OOD detection, Interpretability"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?16404ec2cd2689e8d0f38f73fe0d38f9"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon3.ico?ed0664bb8b662bf21b84ddd264d8c2a9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ckodser.github.io/summaries/Top_down_induction_of_decision_trees_rigorous_guarantees_and_inherent_limitations/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class=" sticky-bottom-footer collection-summaries"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">Arshia</span> Soltani Moakhar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Top-down induction of decision trees- rigorous guarantees and inherent limitations</h1> <p class="post-description">greedily learn a decision tree based on the most inflouential variables in all leaves.</p> <a href="https://arxiv.org/abs/1911.07375" style="display: inline-block; padding: 8px 0; font-weight: 500; text-decoration: none; border-bottom: 2px solid transparent; transition: border-color 0.3s ease; background: linear-gradient(135deg, #5e72e4, #ff7eb3); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;" onmouseover="this.style.borderBottom='2px solid #ff7eb3'" onmouseout="this.style.borderBottom='2px solid transparent'" target="_blank" rel="noopener noreferrer">Read Paper →</a> </header> <article> <h3 id="notations-and-definitions">Notations and Definitions</h3> <p><strong>Bare Decision Tree</strong>: \(T^\circ\) is a bare decision tree which means it has unlabeled leaves).</p> <p><strong>influence</strong>: \(\text{Inf}_{i}(f)\) is the influence of variable \(x_i\) on \(f\), defined as:</p> \[\text{Inf}_{i}(f) := \text{Pr}_{x \sim \{0,1\}^n}[f(x) \ne f(x^{\oplus i})]\] <p>with \(x\) drawn uniformly at random and \(x^{\oplus i}\) denoting \(x\) with its \(i\)-th coordinate flipped.</p> <p><strong>Total Influence</strong>: The total influence of a function \(f: \{0,1\}^n \rightarrow \{\pm1\}\), denoted \(\text{Inf}(f)\), is defined as:</p> \[\text{Inf}(f) = \sum_{i=1}^n \text{Inf}_{i}(f)\] <p><strong>error of bare tree</strong>: \(\text{error}(T^{\circ}, f)\) is the error of bare tree \(T^{\circ}\) on function \(f\), where we labeled leaves such that the error is minimized.</p> <p><strong>error of function and \(\pm 1\)</strong>: \(\text{error}(f, \pm 1)\) is the error of \(f\) with respect to \(+1\) or \(-1\) whichever best matches \(f\) and gives it a low error.</p> <hr> <h3 id="method-top-down-decision-tree-construction-buildtopdowndt">Method: <strong>Top-Down Decision Tree Construction (BuildTopDownDT)</strong> </h3> <p>The paper proposes and analyzes a heuristic for constructing decision trees for Boolean functions using a top-down approach with rigorous performance bounds. The method, named <strong>BuildTopDownDT</strong>, uses the variable <em>influence</em> as the splitting criterion.</p> <hr> <h3 id="key-steps-in-the-heuristic">Key Steps in the Heuristic</h3> <ol> <li> <strong>Initialization:</strong> <ul> <li>Start with an empty decision tree \(T^\circ\).</li> </ul> </li> <li> <strong>Scoring:</strong> <ul> <li>For each current leaf \(\ell\), calculate the influence of variables in the sub-function \(f_\ell\) represented by that leaf. <ul> <li>The most influential variable at leaf \(\ell\) is denoted \(x_i(\ell)\), where \(\mathrm{Inf}_{i}(f_{\ell}) \geq \mathrm{Inf}_j(f_{\ell})\) for all \(j\).</li> <li>Assign a score to each leaf:</li> </ul> </li> </ul> </li> </ol> \[\text{score}(\ell) = \Pr_{x \sim \{0,1\}^n}[x \text{ reaches } \ell] \cdot \mathrm{Inf}_{i(\ell)}(f_{\ell}) = 2^{-\lvert \ell \rvert} \cdot \mathrm{Inf}_{i(\ell)}(f_{\ell}),\] <p>where \(\lvert \ell \rvert\) is the depth of leaf \(\ell\).</p> <ol> <li> <strong>Splitting:</strong> <ul> <li>Identify the leaf \(\ell^*\) with the highest score and split the tree at this leaf using the variable \(x_{i(\ell^*)}\).</li> </ul> </li> <li> <strong>Stopping Criterion:</strong> <ul> <li>Repeat the scoring and splitting process until the \(f\)-completion of \(T^\circ\) is an \(\varepsilon\)-approximation of \(f\), where the error is bounded by \(\varepsilon\). \(f\)-completion means assigning the best label to each leaf of \(T^\circ\) such that it is best matched by \(f\).</li> </ul> </li> </ol> <hr> <h4 id="lower-bounds">Lower Bounds:</h4> <ul> <li>For any error parameter \(\varepsilon \in (0, \frac{1}{2})\), there exists a function \(f\) with decision tree size \(s \leq 2^{\tilde{O}(\sqrt{n})}\) such that the heuristic produces a decision tree of size:</li> </ul> \[s^{\Omega\left(\log \tilde{s}\right)}.\] <h4 id="upper-bounds">Upper Bounds:</h4> <p><strong>Theorem 3 (Upper bound for approximate representation)</strong> For every \(f\) with decision tree size \(s\) and every \(\epsilon \in (0, \frac{1}{2})\), the heuristic constructs an approximate decision tree of size at most \(s^{O(\log(s/\epsilon) \log(1/\epsilon))}\).</p> <p><strong>Proof:</strong></p> <p>The proof of this theorem relies on tracking a “cost” metric of the bare tree \(T^{\circ}\) being built by the above algorithm. The heuristic terminates when the \(f\)-completion of \(T^{\circ}\) is an \(\epsilon\)-approximation of \(f\).</p> <p><strong>1. Definition and Properties of “Cost”</strong> The cost of a bare tree \(T^{\circ}\) relative to a function \(f: \{0,1\}^n \rightarrow \{\pm1\}\) is defined as:</p> \[\text{cost}_f(T^{\circ}) = \sum_{\text{leaf } \ell \in T^{\circ}} 2^{-\lvert \ell \rvert} \cdot \text{Inf}(f_\ell)\] <p>where \(\lvert \ell \rvert\) is the depth of leaf \(\ell\) and \(f_\ell\) is the restriction of \(f\) by the path leading to \(\ell\).</p> <p>Lemma 5.1 states the following properties of the cost: First \(\text{error}(T^{\circ}, f) \le \text{cost}_f(T^{\circ})\). This means if the cost drops below \(\epsilon\), the heuristic can terminate.</p> <p>Second when a leaf \(\ell\) in \(T^{\circ}\) is replaced by a query to variable \(x_i\), resulting in a new bare tree \((T^{\circ})'\), the cost decreases by the score of the leaf: \(\text{cost}_{f}((T^{\circ})') = \text{cost}_{f}(T^{\circ}) - 2^{-\lvert \ell \rvert} \cdot \text{Inf}_{i}(f_{\ell})\). The score of a leaf \(\ell\) is defined as \(2^{-\lvert \ell \rvert} \cdot \text{Inf}_{i(\ell)}(f_\ell)\), where \(x_{i(\ell)}\) is the most influential variable of \(f_{\ell}\).</p> <p><strong>2. Lower Bounds on the Score of the Leaf Selected</strong> The heuristic splits the leaf with the highest score. The proof uses two lower bounds on this score.</p> <p>Lemma 5.2 states that at step \(j\), the algorithm selects a leaf \(\ell^*\) with score at least \(\frac{\epsilon}{(j+1)\log(s)}\). This is derived from the fact that if the completion is not an \(\epsilon\)-approximation, there must be a leaf \(\ell\) with \(2^{-\lvert \ell \rvert} \cdot \text{error}(f_{\ell}, \pm 1) &gt; \frac{\epsilon}{j+1}\). Using the relationship \(\text{Var}(g) \ge \text{error}(g, \pm 1)\) and <a href="https://arxiv.org/abs/cs/0508071" rel="external nofollow noopener" target="_blank">Paper: Every decision tree has an influential variable</a> (\(\max_{i} \text{Inf}_{i}(f) \ge \frac{\text{Var}(f)}{\log s}\) ), it follows that \(2^{-\lvert \ell \rvert} \cdot \text{Inf}_{i}(f_{\ell}) &gt; \frac{\epsilon}{(j+1)\log(s)}\) for some variable \(x_i\). Since the heuristic picks the leaf with the maximum score, the selected leaf’s score is at least this value.</p> <p>Lemma 5.4 provides a second lower bound, useful when the cost is large. If at step \(j\), \(\text{cost}_f(T^{\circ}) \ge \epsilon \log(4s/\epsilon)\), the selected leaf \(\ell^*\) has score at least \(\frac{\text{cost}_f(T^{\circ})}{(j+1)\log(4s/\epsilon)\log(s)}\). This lemma uses Lemma 5.3, which bounds the total influence of a size-\(s\) decision tree: \(\text{Inf}(f) \le \text{Var}(f) \log(4s/\text{Var}(f))\).</p> <p><strong>3. Proof of Theorem 3</strong> Let \(C_j\) be the cost after \(j\) steps. The size of the resulting tree is \(j+1\) if the algorithm terminates at step \(j\). The algorithm terminates when \(C_j \le \epsilon\). The analysis proceeds in two phases:</p> <p><strong>Phase 1:</strong> Reduce the cost until it is below \(\epsilon \log(4s/\epsilon)\). While \(C_j &gt; \epsilon \log(4s/\epsilon)\), the heuristic selects a leaf with score at least \(\frac{C_j}{(j+1)\log(4s/\epsilon)\log(s)}\) (from Lemma 5.4). From Lemma 5.1, \(C_{j+1} \le C_j - \frac{C_j}{(j+1)\log(4s/\epsilon)\log(s)} = C_j \left(1 - \frac{1}{(j+1)\log(4s/\epsilon)\log(s)}\right)\). After \(k\) steps,</p> \[C_k \le C_0 \prod_{j=1}^k \left(1 - \frac{1}{j\log(4s/\epsilon)\log(s)}\right) \le C_0 \exp\left(-\sum_{j=1}^k \frac{1}{j\log(4s/\epsilon)\log(s)}\right)\] <p>Using \(\sum_{j=1}^k \frac{1}{j} \approx \log k\),</p> \[C_k \le C_0 \exp\left(-\frac{\log k}{\log(4s/\epsilon)\log(s)}\right)\] <p>Since \(C_0 = \text{Inf}(f) \le \log s\), setting \(k = s^{\log(4s/\epsilon)\log(1/\epsilon)}\) ensures \(C_k \le \epsilon \log(4s/\epsilon)\).</p> <p><strong>Phase 2:</strong> Reduce the cost from below \(\epsilon \log(4s/\epsilon)\) to \(\epsilon\). Once \(C_j \le \epsilon \log(4s/\epsilon)\), the heuristic selects a leaf with score at least \(\frac{\epsilon}{(j+1)\log s}\) (from Lemma 5.2). \(C_{j+1} \le C_j - \frac{\epsilon}{(j+1)\log s}\). For \(m &gt; k\), \(C_k - C_m \ge \sum_{j=k+1}^m \frac{\epsilon}{(j+1)\log s} \ge \frac{\epsilon}{\log s}(\log m - \log k)\). To ensure \(C_m \le \epsilon\), we need \(C_k - \epsilon \ge \frac{\epsilon}{\log s}(\log m - \log k)\). Setting \(\log m = \frac{\log s}{\epsilon} (C_k - \epsilon) + \log k\). Using \(C_k \le \epsilon \log(4s/\epsilon)\) and the value of \(k\) from Phase 1, \(m \le s^{2\log(4s/\epsilon)\log(1/\epsilon)}\).</p> <p>The total number of steps is at most \(m\), so the size of the decision tree is at most \(m+1\), which is \(s^{O(\log(s/\epsilon) \log(1/\epsilon))}\).</p> <p>Thus, for any function \(f\) with decision tree size \(s\) and error parameter \(\epsilon \in (0, \frac{1}{2})\), the heuristic constructs an approximate decision tree of size at most \(s^{O(\log(s/\epsilon) \log(1/\epsilon))}\).</p> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ckodser/ckodser.github.io","data-repo-id":"R_kgDOHfm8sw","data-category":"General","data-category-id":"DIC_kwDOHfm8s84ClE4O","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"light","data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © 2026 Arshia Soltani Moakhar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/summary_image_expand.js?de7a891220c8fc9ce98add9ca26b742e"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?f8abf2f636f242d077f24149a0a56c96"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"My Research Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-summaries",title:"Summaries",description:"Paper summaries",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"projects-data-leakage-of-lora-in-federated-training",title:"Data Leakage of LoRA in federated training",description:"This article highlights the potential risks of reconstructing private data from the gradients shared in Federated Learning, especially when using the LoRA finetuning technique.",section:"Projects",handler:()=>{window.location.href="/projects/Attention_is_all_you_need_to_attack/"}},{id:"projects-basedon",title:"BasedOn",description:"Using Learnable If Statements for Interpretability",section:"Projects",handler:()=>{window.location.href="/projects/BasedOn/"}},{id:"projects-sparsity-for-interpretability",title:"sparsity for interpretability",description:"Leveraging sample sparsity to improve interpretability of neural networks",section:"Projects",handler:()=>{window.location.href="/projects/SPADE/"}},{id:"projects-certified-robust-neural-network",title:"Certified Robust Neural Network",description:"Certify Robustness using median neurons",section:"Projects",handler:()=>{window.location.href="/projects/sparse_L_inf_network/"}},{id:"summaries-a-mathematical-framework-for-transformer-circuits",title:"A Mathematical Framework for Transformer Circuits",description:"In Transformers residual stream is the main object and layers read and write from/to it.",section:"Summaries",handler:()=>{window.location.href="/summaries/A_Mathematical_Framework_for_Transformer_Circuits/"}},{id:"summaries-an-overview-of-early-vision-in-inceptionv1",title:"An Overview of Early Vision in InceptionV1",description:"inceptionV1 feature maps of different layers",section:"Summaries",handler:()=>{window.location.href="/summaries/An_Overview_of_Early_Vision_in_InceptionV1/"}},{id:"summaries-clip-dissect-automatic-description-of-neuron-representations",title:"CLIP-Dissect Automatic Description of Neuron Representations",description:"Find concepts that activates a neuron using a image dataset",section:"Summaries",handler:()=>{window.location.href="/summaries/CLIP-Dissect_Automatic_Description_of_Neuron_Representations_in_Deep_Vision_Networks/"}},{id:"summaries-can-large-language-models-explain-their-internal-mechanisms",title:"Can Large Language Models Explain Their Internal Mechanisms?",description:"summary of Can Large Language Models Explain Their Internal Mechanisms?",section:"Summaries",handler:()=>{window.location.href="/summaries/Can_Large_Language_Models_Explain_Their_Internal_Mechanisms/"}},{id:"summaries-chain-of-images-for-intuitively-reasoning",title:"Chain of Images for Intuitively Reasoning",description:"This paper proposes a Chain-of-Images (CoI) method for multimodal models to solve reasoning problems by generating a series of images as intermediate representations, using a Symbolic Multimodal Large Language Model (SyMLLM).",section:"Summaries",handler:()=>{window.location.href="/summaries/Chain_of_Images_for_Intuitively_Reasoning/"}},{id:"summaries-dash-detection-and-assessment-of-systematic-hallucinations-of-vlms",title:"DASH Detection and Assessment of Systematic Hallucinations of VLMs",description:"Make a dataset that VLMs hallucinate and wrongly think things exist in images",section:"Summaries",handler:()=>{window.location.href="/summaries/DASH_Detection_and_Assessment_of_Systematic_Hallucinations_of_VLMs/"}},{id:"summaries-deep-learning-is-not-so-mysterious-or-different",title:"Deep Learning is Not So Mysterious or Different",description:"??",section:"Summaries",handler:()=>{window.location.href="/summaries/Deep_Learning_is_Not_So_Mysterious_or_Different/"}},{id:"summaries-emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task",title:"Emergent World Representations Exploring a Sequence Model Trained on a Synthetic Task",description:"summary of Emergent World Representations  Exploring a Sequence Model Trained on a Synthetic Task",section:"Summaries",handler:()=>{window.location.href="/summaries/Emergent_World_Representations_Exploring_a_Sequence_Model_Trained_on_a_Synthetic_Task/"}},{id:"summaries-every-decision-tree-has-an-influential-variable",title:"Every decision tree has an influential variable",description:"title is self-explanatory",section:"Summaries",handler:()=>{window.location.href="/summaries/Every_decision_tree_has_an_influential_variable/"}},{id:"summaries-interpretability-beyond-feature-attribution-quantitative-testing-with-concept-activation-vectors-tcav",title:"Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV)",description:"summary of Interpretability Beyond Feature Attribution  Quantitative Testing with Concept Activation Vectors (TCAV)",section:"Summaries",handler:()=>{window.location.href="/summaries/Interpretability_Beyond_Feature_Attribution_Quantitative_Testing_with_Concept_Activation_Vectors_(TCAV)/"}},{id:"summaries-llm-latent-reasoning-as-chain-of-superposition",title:"LLM Latent Reasoning as Chain of Superposition",description:"Train an encoder that summarizes reasoning chunks. Then train a latent reasoning model on the summaries it produces from some CoT data.",section:"Summaries",handler:()=>{window.location.href="/summaries/LLM_Latent_Reasoning_as_Chain_of_Superposition/"}},{id:"summaries-llms-are-single-threaded-reasoners-demystifying-the-working-mechanism-of-soft-thinking",title:"LLMs are Single-threaded Reasoners, Demystifying the Working Mechanism of Soft Thinking",description:"Vanilla Soft Thinking pushes the model to the greedy token sampling internally. They showed that the model usually continues to work only with the most probable next token. To mitigate this issue they suggest adding noise to logits and get better performance.",section:"Summaries",handler:()=>{window.location.href="/summaries/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/"}},{id:"summaries-labeling-neural-representations-with-inverse-recognition",title:"Labeling Neural Representations with Inverse Recognition",description:"summary of Labeling Neural Representations with Inverse Recognition",section:"Summaries",handler:()=>{window.location.href="/summaries/Labeling_Neural_Representations_with_Inverse_Recognition/"}},{id:"summaries-learning-decision-trees-from-random-examples",title:"Learning decision trees from random examples",description:"Decision tree learning By Finding Consistent Decision Trees",section:"Summaries",handler:()=>{window.location.href="/summaries/Learning_decision_trees_from_random_examples/"}},{id:"summaries-leveraged-volume-sampling-for-linear-regression",title:"Leveraged volume sampling for linear regression",description:"Active Learning in linear regression with multiplicative error rate bounds",section:"Summaries",handler:()=>{window.location.href="/summaries/Leveraged_volume_sampling_for_linear_regression/"}},{id:"summaries-multiplex-thinking-reasoning-via-token-wise-branch-and-merge",title:"Multiplex Thinking Reasoning via Token wise Branch and Merge",description:"Make soft-thinking a bit random. Then train with GRPO.",section:"Summaries",handler:()=>{window.location.href="/summaries/Multiplex_Thinking_Reasoning_via_Token_wise_Branch_and_Merge/"}},{id:"summaries-physics-of-language-models",title:"Physics of Language Models",description:"Understanding LLMs by training smaller LMs in controlled environment",section:"Summaries",handler:()=>{window.location.href="/summaries/Physics_of_LM/"}},{id:"summaries-progress-measures-for-grokking-via-mechanistic-interpretability",title:"Progress measures for grokking via mechanistic interpretability",description:"summary of Progress measures for grokking via mechanistic interpretability",section:"Summaries",handler:()=>{window.location.href="/summaries/Progress_measures_for_grokking_via_mechanistic_interpretability/"}},{id:"summaries-properly-learning-decision-trees-in-almost-polynomial-time",title:"Properly learning decision trees in almost polynomial time",description:"learning a decision tree for unifrom random data distribution in O(s ^ log(log(s)))",section:"Summaries",handler:()=>{window.location.href="/summaries/Properly_learning-_decision_trees_in_almost_polynomial_time/"}},{id:"summaries-reasoning-within-the-mind-dynamic-multimodal-interleaving-in-latent-space",title:"Reasoning Within the Mind Dynamic Multimodal Interleaving in Latent Space",description:"Training-free latent reasoning. Optimize latent reasoning tokens to maximize model confidence, which correlates with accuracy.",section:"Summaries",handler:()=>{window.location.href="/summaries/Reasoning_Within_the_Mind_Dynamic_Multimodal_Interleaving_in_Latent_Space/"}},{id:"summaries-scaling-latent-reasoning-via-looped-language-models",title:"Scaling Latent Reasoning via Looped Language Models",description:"Training a recurrent reasoning model. Looping the same models over and over again.",section:"Summaries",handler:()=>{window.location.href="/summaries/Scaling_Latent_Reasoning_via_Looped_Language_Models/"}},{id:"summaries-scaling-monosemanticity-extracting-interpretable-features-from-claude-3-sonnet",title:"Scaling Monosemanticity Extracting Interpretable Features from Claude 3 Sonnet",description:"Scale SAE to Claude 3 Sonnet",section:"Summaries",handler:()=>{window.location.href="/summaries/Scaling_Monosemanticity_Extracting_Interpretable_Features_from_Claude_3_Sonnet/"}},{id:"summaries-soft-tokens-hard-truths",title:"Soft Tokens, Hard Truths",description:"They add Gaussian noise to the soft-thinking embeddings, then train with RL using RLOO.",section:"Summaries",handler:()=>{window.location.href="/summaries/Soft_Tokens_Hard_Truths/"}},{id:"summaries-top-down-induction-of-decision-trees-rigorous-guarantees-and-inherent-limitations",title:"Top-down induction of decision trees- rigorous guarantees and inherent limitations",description:"greedily learn a decision tree based on the most inflouential variables in all leaves.",section:"Summaries",handler:()=>{window.location.href="/summaries/Top_down_induction_of_decision_trees_rigorous_guarantees_and_inherent_limitations/"}},{id:"summaries-towards-monosemanticity-decomposing-language-models-with-dictionary-learning",title:"Towards Monosemanticity Decomposing Language Models With Dictionary Learning",description:"How SAE works",section:"Summaries",handler:()=>{window.location.href="/summaries/Towards_Monosemanticity_Decomposing_Language_Models_With_Dictionary_Learning/"}},{id:"summaries-what-do-we-learn-from-inverting-clip-models",title:"What do we learn from inverting CLIP models?",description:"summary of What do we learn from inverting CLIP models?",section:"Summaries",handler:()=>{window.location.href="/summaries/What_do_we_learn_from_inverting_CLIP_models/"}},{id:"summaries-zoom-in-an-introduction-to-circuits",title:"Zoom In An Introduction to Circuits",description:"Investigate Vision Circuits by Studying the Connections between Neurons",section:"Summaries",handler:()=>{window.location.href="/summaries/Zoom_In_An_Introduction_to_Circuits/"}},{id:"summaries-active-learning-survey",title:"Active Learning Survey",description:"Active Learning for Agnostic classification",section:"Summaries",handler:()=>{window.location.href="/summaries/active-survey/"}},{id:"summaries-the-true-sample-complexity-of-active-learning",title:"The True Sample Complexity of Active Learning",description:"A different definition of active learning label complexity",section:"Summaries",handler:()=>{window.location.href="/summaries/true-active/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%72%73%68%69%61.%73%6F%6C%74%61%6E%69%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=2H6Wl4MAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ckodser","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>