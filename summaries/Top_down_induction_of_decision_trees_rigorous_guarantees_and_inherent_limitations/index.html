<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Top-down induction of decision trees- rigorous guarantees and inherent limitations | Arshia Soltani Moakhar </title> <meta name="author" content="Arshia Soltani Moakhar"> <meta name="description" content="greedily learn a decision tree based on the most inflouential variables in all leaves."> <meta name="keywords" content="academic-website, research, Arshia Soltani Moakhar, ckodser, adversarial training, OOD detection, Interpretability"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon3.ico?ed0664bb8b662bf21b84ddd264d8c2a9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ckodser.github.io/summaries/Top_down_induction_of_decision_trees_rigorous_guarantees_and_inherent_limitations/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class=" sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">Arshia</span> Soltani Moakhar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Top-down induction of decision trees- rigorous guarantees and inherent limitations</h1> <p class="post-description">greedily learn a decision tree based on the most inflouential variables in all leaves.</p> <a href="https://arxiv.org/abs/1911.07375" class="btn btn-primary" target="_blank" rel="noopener noreferrer">Link</a> </header> <article> <h3 id="method-top-down-decision-tree-construction-buildtopdowndt">Method: <strong>Top-Down Decision Tree Construction (BuildTopDownDT)</strong> </h3> <p>The paper proposes and analyzes a heuristic for constructing decision trees for Boolean functions using a top-down approach with rigorous performance bounds. The method, named <strong>BuildTopDownDT</strong>, uses the variable <em>influence</em> as the splitting criterion.</p> <hr> <h3 id="key-steps-in-the-heuristic">Key Steps in the Heuristic</h3> <ol> <li> <strong>Initialization:</strong> <ul> <li>Start with an empty decision tree \(T^\circ\) (a bare decision tree is a tree with unlabeled leaves).</li> </ul> </li> <li> <strong>Scoring:</strong> <ul> <li>For each current leaf \(\ell\), calculate the influence of variables in the sub-function \(f_\ell\) represented by that leaf. <ul> <li>The most influential variable at leaf \(\ell\) is denoted \(x_i(\ell)\), where \(\mathrm{Inf}_{i}(f_{\ell}) \geq \mathrm{Inf}_j(f_{\ell})\) for all \(j\).</li> <li>Assign a score to each leaf:</li> </ul> </li> </ul> </li> </ol> \[\text{score}(\ell) = \Pr_{x \sim \{0,1\}^n}[x \text{ reaches } \ell] \cdot \mathrm{Inf}_{i(\ell)}(f_{\ell}) = 2^{-\lvert \ell \rvert} \cdot \mathrm{Inf}_{i(\ell)}(f_{\ell}),\] <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   where $$\lvert \ell \rvert$$ is the depth of leaf $$ \ell $$.
</code></pre></div></div> <ol> <li> <strong>Splitting:</strong> <ul> <li>Identify the leaf \(\ell^*\) with the highest score and split the tree at this leaf using the variable \(x_{i(\ell^*)}\).</li> </ul> </li> <li> <strong>Stopping Criterion:</strong> <ul> <li>Repeat the scoring and splitting process until the \(f\)-completion of \(T^\circ\) is an \(\varepsilon\)-approximation of \(f\), where the error is bounded by \(\varepsilon\). \(f\)-completion means assigning the best label to each leaf of \(T^\circ\) such that it is best matched by \(f\).</li> </ul> </li> </ol> <hr> <h4 id="lower-bounds">Lower Bounds:</h4> <ul> <li>For any error parameter \(\varepsilon \in (0, \frac{1}{2})\), there exists a function \(f\) with decision tree size \(s \leq 2^{\tilde{O}(\sqrt{n})}\) such that the heuristic produces a decision tree of size:</li> </ul> \[s^{\Omega\left(\log \tilde{s}\right)}.\] <h4 id="upper-bounds">Upper Bounds:</h4> <p><strong>Theorem 3 (Upper bound for approximate representation)</strong> For every \(f\) with decision tree size \(s\) and every \(\epsilon \in (0, \frac{1}{2})\), the heuristic constructs an approximate decision tree of size at most \(s^{O(\log(s/\epsilon) \log(1/\epsilon))}\).</p> <p><strong>Proof:</strong></p> <p>The proof of this theorem relies on tracking a “cost” metric of the bare tree \(T^{\circ}\) being built by the above algorithm. The heuristic terminates when the \(f\)-completion of \(T^{\circ}\) is an \(\epsilon\)-approximation of \(f\).</p> <p>The total influence of a function \(f: \{0,1\}^n \rightarrow \{\pm1\}\), denoted \(\text{Inf}(f)\), is defined as:</p> \[\text{Inf}(f) = \sum_{i=1}^n \text{Inf}_{i}(f)\] <p>where \(\text{Inf}_{i}(f)\) is the influence of variable \(x_i\) on \(f\), defined as:</p> \[\text{Inf}_{i}(f) := \text{Pr}_{x \sim \{0,1\}^n}[f(x) \ne f(x^{\oplus i})]\] <p>with \(x\) drawn uniformly at random and \(x^{\oplus i}\) denoting \(x\) with its \(i\)-th coordinate flipped.</p> <p><strong>1. Definition and Properties of “Cost”</strong> The cost of a bare tree \(T^{\circ}\) relative to a function \(f: \{0,1\}^n \rightarrow \{\pm1\}\) is defined as:</p> \[\text{cost}_f(T^{\circ}) = \sum_{\text{leaf } \ell \in T^{\circ}} 2^{-|\ell|} \cdot \text{Inf}(f_\ell)\] <table> <tbody> <tr> <td>where $$</td> <td>\ell</td> <td>\(is the depth of leaf\)\ell\(and\)f_\ell\(is the restriction of\)f\(by the path leading to\)\ell$$.</td> </tr> </tbody> </table> <p>Lemma 5.1 states the following properties of the cost: First \(\text{error}(T^{\circ}, f) \le \text{cost}_f(T^{\circ})\). This means if the cost drops below \(\epsilon\), the heuristic can terminate.</p> <p>Second when a leaf \(\ell\) in \(T^{\circ}\) is replaced by a query to variable \(x_i\), resulting in a new bare tree \((T^{\circ})'\), the cost decreases by the score of the leaf: \(\text{cost}_{f}((T^{\circ})') = \text{cost}_{f}(T^{\circ}) - 2^{-|\ell|} \cdot \text{Inf}_{i}(f_{\ell})\). The score of a leaf \(\ell\) is defined as \(2^{-|\ell|} \cdot \text{Inf}_{i(\ell)}(f_\ell)\), where \(x_{i(\ell)}\) is the most influential variable of \(f_{\ell}\).</p> <p><strong>2. Lower Bounds on the Score of the Leaf Selected</strong> The heuristic splits the leaf with the highest score. The proof uses two lower bounds on this score.</p> <table> <tbody> <tr> <td>Lemma 5.2 states that at step \(j\), the algorithm selects a leaf \(\ell^*\) with score at least \(\frac{\epsilon}{(j+1)\log(s)}\). This is derived from the fact that if the completion is not an \(\epsilon\)-approximation, there must be a leaf \(\ell\) with $$2^{-</td> <td>\ell</td> <td>} \cdot \text{error}(f_{\ell}, \pm 1) &gt; \frac{\epsilon}{j+1}$$.</td> </tr> <tr> <td>Using the relationship \(\text{Var}(g) \ge \text{error}(g, \pm 1)\) and Theorem 12 (\(\max_i \text{Inf}_{i}(f) \ge \frac{\text{Var}(f)}{\log s}\) ), it follows that $$2^{-</td> <td>\ell</td> <td>} \cdot \text{Inf}<em>{i}(f</em>{\ell}) &gt; \frac{\epsilon}{(j+1)\log(s)}\(for some variable\)x_i$$. Since the heuristic picks the leaf with the maximum score, the selected leaf’s score is at least this value.</td> </tr> </tbody> </table> <p>Lemma 5.4 provides a second lower bound, useful when the cost is large. If at step \(j\), \(\text{cost}_f(T^{\circ}) \ge \epsilon \log(4s/\epsilon)\), the selected leaf \(\ell^*\) has score at least \(\frac{\text{cost}_f(T^{\circ})}{(j+1)\log(4s/\epsilon)\log(s)}\). This lemma uses Lemma 5.3, which bounds the total influence of a size-\(s\) decision tree: \(\text{Inf}(f) \le \text{Var}(f) \log(4s/\text{Var}(f))\).</p> <p><strong>3. Proof of Theorem 3</strong> Let \(C_j\) be the cost after \(j\) steps. The size of the resulting tree is \(j+1\) if the algorithm terminates at step \(j\). The algorithm terminates when \(C_j \le \epsilon\). The analysis proceeds in two phases:</p> <p><strong>Phase 1:</strong> Reduce the cost until it is below \(\epsilon \log(4s/\epsilon)\). While \(C_j &gt; \epsilon \log(4s/\epsilon)\), the heuristic selects a leaf with score at least \(\frac{C_j}{(j+1)\log(4s/\epsilon)\log(s)}\) (from Lemma 5.4). From Lemma 5.1, \(C_{j+1} \le C_j - \frac{C_j}{(j+1)\log(4s/\epsilon)\log(s)} = C_j \left(1 - \frac{1}{(j+1)\log(4s/\epsilon)\log(s)}\right)\). After \(k\) steps,</p> \[C_k \le C_0 \prod_{j=1}^k \left(1 - \frac{1}{j\log(4s/\epsilon)\log(s)}\right) \le C_0 \exp\left(-\sum_{j=1}^k \frac{1}{j\log(4s/\epsilon)\log(s)}\right)\] <p>Using \(\sum_{j=1}^k \frac{1}{j} \approx \log k\),</p> \[C_k \le C_0 \exp\left(-\frac{\log k}{\log(4s/\epsilon)\log(s)}\right)\] <p>Since \(C_0 = \text{Inf}(f) \le \log s\), setting \(k = s^{\log(4s/\epsilon)\log(1/\epsilon)}\) ensures \(C_k \le \epsilon \log(4s/\epsilon)\).</p> <p><strong>Phase 2:</strong> Reduce the cost from below \(\epsilon \log(4s/\epsilon)\) to \(\epsilon\). Once \(C_j \le \epsilon \log(4s/\epsilon)\), the heuristic selects a leaf with score at least \(\frac{\epsilon}{(j+1)\log s}\) (from Lemma 5.2). \(C_{j+1} \le C_j - \frac{\epsilon}{(j+1)\log s}\). For \(m &gt; k\), \(C_k - C_m \ge \sum_{j=k+1}^m \frac{\epsilon}{(j+1)\log s} \ge \frac{\epsilon}{\log s}(\log m - \log k)\). To ensure \(C_m \le \epsilon\), we need \(C_k - \epsilon \ge \frac{\epsilon}{\log s}(\log m - \log k)\). Setting \(\log m = \frac{\log s}{\epsilon} (C_k - \epsilon) + \log k\). Using \(C_k \le \epsilon \log(4s/\epsilon)\) and the value of \(k\) from Phase 1, \(m \le s^{2\log(4s/\epsilon)\log(1/\epsilon)}\).</p> <p>The total number of steps is at most \(m\), so the size of the decision tree is at most \(m+1\), which is \(s^{O(\log(s/\epsilon) \log(1/\epsilon))}\).</p> <p>Thus, for any function \(f\) with decision tree size \(s\) and error parameter \(\epsilon \in (0, \frac{1}{2})\), the heuristic constructs an approximate decision tree of size at most \(s^{O(\log(s/\epsilon) \log(1/\epsilon))}\).</p> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ckodser/ckodser.github.io","data-repo-id":"R_kgDOHfm8sw","data-category":"General","data-category-id":"DIC_kwDOHfm8s84ClE4O","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"light","data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © 2025 Arshia Soltani Moakhar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"My Research Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Repositories to which I&#39;m a major contributor. Most of these works were done as a course final project.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-summaries",title:"Summaries",description:"Paper summaries",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"projects-data-leakage-of-lora-in-federated-training",title:"Data Leakage of LoRA in federated training",description:"This article highlights the potential risks of reconstructing private data from the gradients shared in Federated Learning, especially when using the LoRA finetuning technique.",section:"Projects",handler:()=>{window.location.href="/projects/Attention_is_all_you_need_to_attack/"}},{id:"projects-basedon",title:"BasedOn",description:"Using Learnable If Statements for Interpretability",section:"Projects",handler:()=>{window.location.href="/projects/BasedOn/"}},{id:"projects-sparsity-for-interpretability",title:"sparsity for interpretability",description:"Leveraging sample sparsity to improve interpretability of neural networks",section:"Projects",handler:()=>{window.location.href="/projects/SPADE/"}},{id:"projects-certified-robust-neural-network",title:"Certified Robust Neural Network",description:"Certify Robustness using median neurons",section:"Projects",handler:()=>{window.location.href="/projects/sparse_L_inf_network/"}},{id:"summaries-a-mathematical-framework-for-transformer-circuits",title:"A Mathematical Framework for Transformer Circuits",description:"In Transformers residual stream is the main object and layers read and write from/to it.",section:"Summaries",handler:()=>{window.location.href="/summaries/A_Mathematical_Framework_for_Transformer_Circuits/"}},{id:"summaries-an-overview-of-early-vision-in-inceptionv1",title:"An Overview of Early Vision in InceptionV1",description:"inceptionV1 feature maps of different layers",section:"Summaries",handler:()=>{window.location.href="/summaries/An_Overview_of_Early_Vision_in_InceptionV1/"}},{id:"summaries-clip-dissect-automatic-description-of-neuron-representations",title:"CLIP-Dissect Automatic Description of Neuron Representations",description:"Find concepts that activates a neuron using a image dataset",section:"Summaries",handler:()=>{window.location.href="/summaries/CLIP-Dissect_Automatic_Description_of_Neuron_Representations_in_Deep_Vision_Networks/"}},{id:"summaries-can-large-language-models-explain-their-internal-mechanisms",title:"Can Large Language Models Explain Their Internal Mechanisms?",description:"summary of Can Large Language Models Explain Their Internal Mechanisms?",section:"Summaries",handler:()=>{window.location.href="/summaries/Can_Large_Language_Models_Explain_Their_Internal_Mechanisms/"}},{id:"summaries-emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task",title:"Emergent World Representations Exploring a Sequence Model Trained on a Synthetic Task",description:"summary of Emergent World Representations  Exploring a Sequence Model Trained on a Synthetic Task",section:"Summaries",handler:()=>{window.location.href="/summaries/Emergent_World_Representations_Exploring_a_Sequence_Model_Trained_on_a_Synthetic_Task/"}},{id:"summaries-interpretability-beyond-feature-attribution-quantitative-testing-with-concept-activation-vectors-tcav",title:"Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV)",description:"summary of Interpretability Beyond Feature Attribution  Quantitative Testing with Concept Activation Vectors (TCAV)",section:"Summaries",handler:()=>{window.location.href="/summaries/Interpretability_Beyond_Feature_Attribution_Quantitative_Testing_with_Concept_Activation_Vectors_(TCAV)/"}},{id:"summaries-labeling-neural-representations-with-inverse-recognition",title:"Labeling Neural Representations with Inverse Recognition",description:"summary of Labeling Neural Representations with Inverse Recognition",section:"Summaries",handler:()=>{window.location.href="/summaries/Labeling_Neural_Representations_with_Inverse_Recognition/"}},{id:"summaries-leveraged-volume-sampling-for-linear-regression",title:"Leveraged volume sampling for linear regression",description:"Active Learning in linear regression with multiplicative error rate bounds",section:"Summaries",handler:()=>{window.location.href="/summaries/Leveraged_volume_sampling_for_linear_regression/"}},{id:"summaries-progress-measures-for-grokking-via-mechanistic-interpretability",title:"Progress measures for grokking via mechanistic interpretability",description:"summary of Progress measures for grokking via mechanistic interpretability",section:"Summaries",handler:()=>{window.location.href="/summaries/Progress_measures_for_grokking_via_mechanistic_interpretability/"}},{id:"summaries-properly-learning-decision-trees-in-almost-polynomial-time",title:"Properly learning decision trees in almost polynomial time",description:"learning a decision tree for unifrom random data distribution in O(s ^ log(log(s)))",section:"Summaries",handler:()=>{window.location.href="/summaries/Properly_learning-_decision_trees_in_almost_polynomial_time/"}},{id:"summaries-scaling-monosemanticity-extracting-interpretable-features-from-claude-3-sonnet",title:"Scaling Monosemanticity Extracting Interpretable Features from Claude 3 Sonnet",description:"Scale SAE to Claude 3 Sonnet",section:"Summaries",handler:()=>{window.location.href="/summaries/Scaling_Monosemanticity_Extracting_Interpretable_Features_from_Claude_3_Sonnet/"}},{id:"summaries-top-down-induction-of-decision-trees-rigorous-guarantees-and-inherent-limitations",title:"Top-down induction of decision trees- rigorous guarantees and inherent limitations",description:"greedily learn a decision tree based on the most inflouential variables in all leaves.",section:"Summaries",handler:()=>{window.location.href="/summaries/Top_down_induction_of_decision_trees_rigorous_guarantees_and_inherent_limitations/"}},{id:"summaries-towards-monosemanticity-decomposing-language-models-with-dictionary-learning",title:"Towards Monosemanticity Decomposing Language Models With Dictionary Learning",description:"How SAE works",section:"Summaries",handler:()=>{window.location.href="/summaries/Towards_Monosemanticity_Decomposing_Language_Models_With_Dictionary_Learning/"}},{id:"summaries-what-do-we-learn-from-inverting-clip-models",title:"What do we learn from inverting CLIP models?",description:"summary of What do we learn from inverting CLIP models?",section:"Summaries",handler:()=>{window.location.href="/summaries/What_do_we_learn_from_inverting_CLIP_models/"}},{id:"summaries-zoom-in-an-introduction-to-circuits",title:"Zoom In An Introduction to Circuits",description:"Investigate Vision Circuits by Studying the Connections between Neurons",section:"Summaries",handler:()=>{window.location.href="/summaries/Zoom_In_An_Introduction_to_Circuits/"}},{id:"summaries-active-learning-survey",title:"Active Learning Survey",description:"Active Learning for Agnostic classification",section:"Summaries",handler:()=>{window.location.href="/summaries/active-survey/"}},{id:"summaries-the-true-sample-complexity-of-active-learning",title:"The True Sample Complexity of Active Learning",description:"A different definition of active learning label complexity",section:"Summaries",handler:()=>{window.location.href="/summaries/true-active/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%72%73%68%69%61.%73%6F%6C%74%61%6E%69%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ckodser","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>