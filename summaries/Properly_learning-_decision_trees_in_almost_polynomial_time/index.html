<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Properly learning decision trees in almost polynomial time | Arshia Soltani Moakhar </title> <meta name="author" content="Arshia Soltani Moakhar"> <meta name="description" content="learning a decision tree for unifrom random data distribution in O(s ^ log(log(s)))"> <meta name="keywords" content="academic-website, research, Arshia Soltani Moakhar, ckodser, adversarial training, OOD detection, Interpretability"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?16404ec2cd2689e8d0f38f73fe0d38f9"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon3.ico?ed0664bb8b662bf21b84ddd264d8c2a9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ckodser.github.io/summaries/Properly_learning-_decision_trees_in_almost_polynomial_time/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class=" sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">Arshia</span> Soltani Moakhar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Properly learning decision trees in almost polynomial time</h1> <p class="post-description">learning a decision tree for unifrom random data distribution in O(s ^ log(log(s)))</p> <a href="https://arxiv.org/abs/2109.00637" style="display: inline-block; padding: 8px 0; font-weight: 500; text-decoration: none; border-bottom: 2px solid transparent; transition: border-color 0.3s ease; background: linear-gradient(135deg, #5e72e4, #ff7eb3); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;" onmouseover="this.style.borderBottom='2px solid #ff7eb3'" onmouseout="this.style.borderBottom='2px solid transparent'" target="_blank" rel="noopener noreferrer">Read Paper →</a> </header> <article> <h3 id="definitions"><strong>Definitions</strong></h3> <h4 id="1-decision-tree"><strong>1. Decision Tree</strong></h4> <ul> <li> <strong>Size of the tree</strong>: The number of leaves in the tree.</li> <li> <strong>Depth of the tree</strong>: The length of the longest root-to-leaf path.</li> <li> <p><strong>Average Depth</strong>: Defined as:</p> <p>\(\Delta(T) := E_{x \sim \{\pm1\}^n}[\text{Depth of leaf that } x \text{ reaches}] =\text{for uniform data distribution}: \sum_{\text{leaves } \ell \in T} 2^{-\text{depth}(\ell)} \cdot \text{depth}(\ell).\) For a size-\(s\) tree (on uniform data distribution), \(\Delta(T) \leq \log s\).</p> </li> <li> <strong>A restriction</strong>: A restriction \(\pi\) of a function \(f : \{\pm1\}^n \to \{\pm1\}\), denoted \(f_\pi\), is the subfunction of \(f\) that one obtains by fixing a subset of the variables to constants (i.e. \(x_i = b\) for \(i \in [n]\) and \(b \in \{\pm1\}\)). We write \(\lvert \pi \rvert\) to denote the number of variables fixed by \(\pi\).</li> <li>We define \(n\) as the number of input dimensions.</li> </ul> <hr> <h4 id="2-influence-of-a-variable"><strong>2. Influence of a Variable</strong></h4> <p>The <strong>influence</strong> of a variable \(x_i\) with respect to a function \(f : \{\pm1\}^n \to \{\pm1\}\) is defined as:</p> <p>\(\text{Inf}_i(f) := \Pr_{x \sim \{\pm1\}^n}[f(x) \neq f(x^{\sim i})],\) where \(x^{\sim i}\) denotes \(x\) with the \(i\)-th variable rerandomized (flipped with probability \(\frac{1}{2}\)).</p> <hr> <h3 id="pruning-a-decision-tree"><strong>Pruning a Decision Tree</strong></h3> <p>Our Goal is to find a pruned version of our decision tree such that</p> <ol> <li>The size and depth of the pruned tree are no larger than the original.</li> <li>The pruned tree is <strong>everywhere \(\tau\)-influential</strong>, meaning every variable queried at each node in the tree has influence at least \(\tau\).</li> </ol> <h4 id="theorem-4-pruning-lemma-for-the-realizable-setting"><strong>Theorem 4 (Pruning Lemma for the Realizable Setting)</strong></h4> <p>Let \(f\) be computable by a size-\(s\) decision tree \(T\), and let \(\tau &gt; 0\). There exists a <strong>pruning</strong> \(\textbf{T}^*\) of \(T\) satisfying the following:</p> <ol> <li> <strong>Error Bound</strong>: \(\Pr_{x \sim \{\pm1\}^n}\big[f(x) \neq T^*(x)\big] \leq \tau \log s.\)</li> <li> <strong>Influence of Variables</strong>: For every node \(v\) in \(T^*\), let \(i(v)\) denote the variable queried at \(v\). Then: \(\text{Inf}_{i(v)}(f_v) \geq \tau,\) where \(f_v\) is the restriction of \(f\) along the root-to-\(v\) path in \(T^*\).</li> </ol> <blockquote> <p>This is because start from the root and do pruning recursively, first prune the root if it is not influential, then recursively prune the left subtree, then prune the right subtree. There are two cases:</p> <ol> <li>If the root is \(\tau\)-influential:</li> </ol> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>If the root is $$\tau$$-influential, then we prune left and right subtree recursively. When both sides are $$\tau$$-influential, and the root is also $$\tau$$-influential, then the whole tree is $$\tau$$-influential.
The error of the resulting tree is equal to
$$\Pr_{x \sim \{\pm1\}^n}\big[f(x) \neq T^*(x)\big]   =$$
Defining $$P_l:=\Pr_{x \sim \{\pm1\}^n}[x_{i(\text{root})}=-1]$$ and $$P_r:=\Pr_{x \sim \{\pm1\}^n}[x_{i(\text{root})}=1]$$, we have
$$P_l\cdot \Pr_{x \sim \{\pm1\}^n \lvert x_{i(\text{root})}=-1}\big[f(x) \neq T^*_l(x)\big] + P_r\cdot \Pr_{x \sim \{\pm1\}^n  \rvert x_{i(\text{root})}=+1}\big[f(x) \neq T^*_r(x)\big]$$
From the theorem we know that pruning subtrees is accurate so the error of the resulting tree is less than
$$\leq P_l\cdot \tau \log s + P_r\cdot \tau \log s = \tau \log s$$ &gt; 2.  If the root is not $$\tau$$-influential: We have to remove the root along one of its subtrees and then recursively prune the remaining subtree.
It is clear that the resulting tree is $$\tau$$-influential. We calculate the error of the resulting tree in two steps. Assume we remove left subtree. $$x^{!i}$$ denotes $$x$$ with the $$i$$-th variable flipped.
$$\Pr_{x \sim \{\pm1\}^n}\big[f(x) \neq T^*(x)\big]   =$$
Defining $$P_l:=\Pr_{x \sim \{\pm1\}^n}[x_{i(\text{root})}=-1]$$ and $$P_r:=\Pr_{x \sim \{\pm1\}^n}[x_{i(\text{root})}=1]$$, we have
$$P_l\cdot \Pr_{x \sim \{\pm1\}^n \lvert x_{i(\text{root})}=-1}\big[f(x) \neq T^*_r(x^{!i})\big] + P_r\cdot \Pr_{x \sim \{\pm1\}^n  \rvert x_{i(\text{root})}=+1}\big[f(x) \neq T^*_r(x)\big]$$
$$\leq P_l\cdot \Pr_{x \sim \{\pm1\}^n \mid x_{i(\text{root})}=-1}\big[f(x) \neq T^*_r(x^{!i})\big] + P_r\cdot \tau \log s$$
cases where $$f(x) \neq T^*_r(x^{!i})$$ are two groups.
1. $$f(x) = f(x^{!i})$$ so $$f(x^{!i}) \neq T^*_r(x^{!i})$$ These samples should not be that large since $$f_r(x)$$ is approximate by $$T^*_r(x)$$.
$$ \Pr_{x \sim \{\pm1\}^n \mid x_{i(\text{root})}=-1}\big[f(x) \neq T^*_r(x^{!i})\big] \text{ and }   \big[f(x) = f(x^{!i})\big] \leq $$
$$ \Pr_{x \sim \{\pm1\}^n \mid x_{i(\text{root})}=-1} \big[f(x^{!i}) \neq T^*_r(x^{!i})]\big] \leq \tau \log s $$
2. $$f(x) \neq f(x^{!i})$$ so $$f(x^{!i}) = T^*_r(x^{!i})$$
These are the samples that effected the influence of the root, so intuitively since the root influence is low, their number should not be too large.
$$ \Pr_{x \sim \{\pm1\}^n \mid x_{i(\text{root})}=-1}\big[f(x) \neq T^*_r(x^{!i})\big] \text{ and } \big[f(x) \neq f(x^{!i})\big] \leq $$
$$ \Pr_{x \sim \{\pm1\}^n \mid x_{i(\text{root})}=-1} \big[f(x) \neq f(x^{!i})\big] = $$
if $$f(x) \neq f(x^{!i})$$ then $$f(x^{!i}) \neq f(x)$$ so it is equal to
$$ \Pr_{x \sim \{\pm1\}^n} \big[f(x) \neq f(x^{!i})\big] = 2\text{inf}_{i}(f)$$
since we don't rerandomize but force the change.
combining these we have the error is less than
 $$\leq P_l\cdot (2\text{inf}_{i}(f) + \tau \log (s/2)) + P_r\cdot \tau \log (s/2) \leq$$
 $$\leq P_l\cdot (2\tau + \tau \log (s/2)) + P_r\cdot \tau \log (s/2)$$
Given that $$P_l=P_r=1/2$$, we have
$$= \tau+ \tau \log (s/2) = \tau \log s$$
</code></pre></div></div> <hr> <h4 id="how-it-is-used"><strong>How it is used?</strong></h4> <p><strong>Bound on Influential Variables</strong>: In the realizable setting + uniform data distribution: - The total influence of a size-\(s\) decision tree is bounded by \(\log s\).</p> <blockquote> <p>This is because \(\text{total influence} = \sum_i \text{Inf}_i(f) = \sum_i E_{x \sim \{\pm1\}^n}[ f(x) \neq f(x^{\sim i})]=E_{x \sim \{\pm1\}^n}[\sum_i f(x) \neq f(x^{\sim i})] =\) if \(i\) is not queried in any node from root to leaf \(x\) reaches then \(f(x) = f(x^{\sim i})\). So it is equal to \(E_{x \sim \{\pm1\}^n}[\sum_{i \in [i(v) \text{ for } v \text{ in path root to leaf that } x \text{ reaches}] } f(x) \neq f(x^{\sim i})]\) If we think about the effect of each \(v\) in this we have \(\sum_{v \in \text{Tree}} P_{x \sim \{\pm1\}^n }[x \text{ reaches } v] \cdot E_{x \sim \text{subtree of } v }[ f(x) \neq f(x^{\sim i(v)})] = \sum_{v \in \text{Tree}} P_{x \sim \{\pm1\}^n }[x \text{ reaches } v] \cdot \text{inf}_{i(v)}(f_v)\) Since \(\text{inf}\) is always less than \(1\) \(\leq \sum_{v \in \text{Tree}} P_{x \sim \{\pm1\}^n }[x \text{ reaches } v]\) If we switch back and check how many nodes a sample has effect on we get \(= E_{x \sim \{\pm1\}^n}[\text{Depth of leaf that } x \text{ reaches}] = \Delta(f)\) For unifrom data distribution this is less than \(\log s\).</p> </blockquote> <ul> <li>Therefore, the number of variables with influence \(\geq \tau\) is at most \((\log s)/\tau\).</li> <li>If we set \(\tau=\frac{\epsilon}{\log s}\), Then using theorem 4, there exists a decision tree where each node is \(\frac{\epsilon}{\log s}\)-influential and the error of the tree is less than \(\epsilon\). We try to find that recursively.</li> <li>In each step, we need to find the best tree on \(f_\pi\) and has a size less than \(s\) for all those sizes. For each of these we need to check \((\log s)/\tau = \frac{\log^2 s}{\epsilon}\) dimensions.</li> <li>We can easily remove any node with depth more than \(d=\log(s/\epsilon)\). This is because the error of doing so would be at most \(\epsilon\).So the run time will be:</li> </ul> <blockquote> <p>There is an algorithm which, given as input \(\varepsilon &gt; 0\), \(s \in \mathbb{N}\), and query access to a size-\(s\) decision tree \(f : \{\pm 1\}^n \to \{\pm 1\}\), runs in time \(\widetilde{O}(n^2) \cdot \left(\frac{s}{\varepsilon} \right)^{O\left(\log \left(\frac{\log s}{\varepsilon} \right)\right)}\) and outputs a size-\(s\) decision tree hypothesis \(T\) that, with high probability, satisfies \(\mathrm{dist}(T, f) \leq \varepsilon.\)</p> </blockquote> <p>The proof of the runtime algorithm is as follows:</p> <blockquote> <p>The number of restrictions is \(n\cdot\sum_{k=1}^{d} \left(\frac{\log^2 s}{\epsilon} \right)^k = n \cdot\left(\frac{\log^2 s}{\epsilon} \right)^{O(d)}.\) We assumed that variable influences can be computed exactly in unit time, whereas in actuality, we can only obtain estimates of these quantities via random sampling. By inspection of our proofs, it suffices for these estimates to be accurate to \(\pm\tau/2\). Query access to \(f\) provides us with query access to \(f_\pi\) for any \(\pi\), and hence by the Chernoff bound, we can estimate \(\text{Inf}_i(f_\pi)\) to accuracy \(\pm \tau/2\) and with confidence \(1-\delta\) using \(O(\log(1/\delta)/\tau^2)\) queries and in \(O(\log(1/\delta)/\tau^2)\) time. The number of times variables’ influences are computed throughout the execution of the algorithm is at most \(n \cdot\left( \frac{\log^2 s}{\epsilon} \right)^{O(d)}\) and so by setting \(\delta &lt; 1/\left(n \cdot ((\log^2 s)/\epsilon)^{O(d)}\right)\), we ensure that w.h.p. all our estimates are indeed accurate to within \(\pm \tau/2\). The overall runtime of our algorithm is \(n \cdot s^2 \cdot \left(\frac{\log^2 s}{\epsilon}\right)^{O(d)} \cdot \frac{n \log^2 s}{\epsilon^2} \left(\log n + d \log \left(\frac{\log^2 s}{\epsilon} \right)\right)=\) \(n \cdot s^2 \cdot \left( \frac{\log^2 s}{\epsilon}\right)^{O(\log(s / \epsilon))} \cdot \frac{n \log^2 s}{\epsilon^2} \left( \log n + \log(s / \epsilon) \log \left( \frac{\log^2 s}{\epsilon}\right)\right)=\) \(n^2 \cdot \frac{s^2}{\epsilon} \cdot \left( \frac{\log^2 s}{\epsilon}\right)^{O(\log(s / \epsilon))} \cdot \left( \log n + \log(s / \epsilon) \log \left( \frac{\log^2 s}{\epsilon}\right)\right)\leq\) \(\tilde{O}\left( n^2 \cdot \frac{s^2}{\epsilon} \cdot \exp\left( \log\left(\frac{\log^2 s}{\epsilon}\right)\cdot O(\log(s / \epsilon))\right) \right) \leq\) \(\tilde{O}\left( n^2 \cdot \frac{s^2}{\epsilon} \cdot \exp\left( (2\log(\log s)- \log(\epsilon)) \cdot O(\log(s / \epsilon))\right) \right)\) Somehow it simplifies to \(\leq \tilde{O}\left( n^2 \cdot \frac{s}{\epsilon} \cdot s^{O(\log (\log s)/\epsilon)}\right),\) and this completes the proof.</p> </blockquote> <hr> <h4 id="agnostic-setting"> <strong>Agnostic Setting</strong>:</h4> <h3 id="extension-to-the-agnostic-setting"><strong>Extension to the Agnostic Setting</strong></h3> <p>In the agnostic setting (where \(f\) may not exactly correspond to a size-\(s\) decision tree):</p> <ul> <li>The number of influential variables is no longer bounded by \((\log s)/\tau\). It can be as large as \(\Omega(n)\).</li> <li>The algorithm employs <strong>smoothing</strong> of \(f\) and <strong>noisy influence</strong> measures: <ul> <li>After smoothing \(f\) with noise parameter \(\delta\), the smoothed function \(\tilde{f}\) is \((\delta \log s)\)-close to \(f\), and the set of variables with noisy influence \(\geq \tau\) is bounded by \(1/(\tau \delta)\).</li> </ul> </li> </ul> <p>These ideas enable an agnostic algorithm that achieves accuracy \(O(\text{opt}) + \epsilon\).</p> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ckodser/ckodser.github.io","data-repo-id":"R_kgDOHfm8sw","data-category":"General","data-category-id":"DIC_kwDOHfm8s84ClE4O","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"light","data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © 2026 Arshia Soltani Moakhar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?f8abf2f636f242d077f24149a0a56c96"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"My Research Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-summaries",title:"Summaries",description:"Paper summaries",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"projects-data-leakage-of-lora-in-federated-training",title:"Data Leakage of LoRA in federated training",description:"This article highlights the potential risks of reconstructing private data from the gradients shared in Federated Learning, especially when using the LoRA finetuning technique.",section:"Projects",handler:()=>{window.location.href="/projects/Attention_is_all_you_need_to_attack/"}},{id:"projects-basedon",title:"BasedOn",description:"Using Learnable If Statements for Interpretability",section:"Projects",handler:()=>{window.location.href="/projects/BasedOn/"}},{id:"projects-sparsity-for-interpretability",title:"sparsity for interpretability",description:"Leveraging sample sparsity to improve interpretability of neural networks",section:"Projects",handler:()=>{window.location.href="/projects/SPADE/"}},{id:"projects-certified-robust-neural-network",title:"Certified Robust Neural Network",description:"Certify Robustness using median neurons",section:"Projects",handler:()=>{window.location.href="/projects/sparse_L_inf_network/"}},{id:"summaries-a-mathematical-framework-for-transformer-circuits",title:"A Mathematical Framework for Transformer Circuits",description:"In Transformers residual stream is the main object and layers read and write from/to it.",section:"Summaries",handler:()=>{window.location.href="/summaries/A_Mathematical_Framework_for_Transformer_Circuits/"}},{id:"summaries-an-overview-of-early-vision-in-inceptionv1",title:"An Overview of Early Vision in InceptionV1",description:"inceptionV1 feature maps of different layers",section:"Summaries",handler:()=>{window.location.href="/summaries/An_Overview_of_Early_Vision_in_InceptionV1/"}},{id:"summaries-clip-dissect-automatic-description-of-neuron-representations",title:"CLIP-Dissect Automatic Description of Neuron Representations",description:"Find concepts that activates a neuron using a image dataset",section:"Summaries",handler:()=>{window.location.href="/summaries/CLIP-Dissect_Automatic_Description_of_Neuron_Representations_in_Deep_Vision_Networks/"}},{id:"summaries-can-large-language-models-explain-their-internal-mechanisms",title:"Can Large Language Models Explain Their Internal Mechanisms?",description:"summary of Can Large Language Models Explain Their Internal Mechanisms?",section:"Summaries",handler:()=>{window.location.href="/summaries/Can_Large_Language_Models_Explain_Their_Internal_Mechanisms/"}},{id:"summaries-dash-detection-and-assessment-of-systematic-hallucinations-of-vlms",title:"DASH Detection and Assessment of Systematic Hallucinations of VLMs",description:"Make a dataset that VLMs hallucinate and wrongly think things exist in images",section:"Summaries",handler:()=>{window.location.href="/summaries/DASH_Detection_and_Assessment_of_Systematic_Hallucinations_of_VLMs/"}},{id:"summaries-deep-learning-is-not-so-mysterious-or-different",title:"Deep Learning is Not So Mysterious or Different",description:"??",section:"Summaries",handler:()=>{window.location.href="/summaries/Deep_Learning_is_Not_So_Mysterious_or_Different/"}},{id:"summaries-emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task",title:"Emergent World Representations Exploring a Sequence Model Trained on a Synthetic Task",description:"summary of Emergent World Representations  Exploring a Sequence Model Trained on a Synthetic Task",section:"Summaries",handler:()=>{window.location.href="/summaries/Emergent_World_Representations_Exploring_a_Sequence_Model_Trained_on_a_Synthetic_Task/"}},{id:"summaries-every-decision-tree-has-an-influential-variable",title:"Every decision tree has an influential variable",description:"title is self-explanatory",section:"Summaries",handler:()=>{window.location.href="/summaries/Every_decision_tree_has_an_influential_variable/"}},{id:"summaries-interpretability-beyond-feature-attribution-quantitative-testing-with-concept-activation-vectors-tcav",title:"Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV)",description:"summary of Interpretability Beyond Feature Attribution  Quantitative Testing with Concept Activation Vectors (TCAV)",section:"Summaries",handler:()=>{window.location.href="/summaries/Interpretability_Beyond_Feature_Attribution_Quantitative_Testing_with_Concept_Activation_Vectors_(TCAV)/"}},{id:"summaries-llm-latent-reasoning-as-chain-of-superposition",title:"LLM Latent Reasoning as Chain of Superposition",description:"Train an encoder that summarizes reasoning chunks. Then train a latent reasoning model on the summaries it produces from some CoT data.",section:"Summaries",handler:()=>{window.location.href="/summaries/LLM_Latent_Reasoning_as_Chain_of_Superposition/"}},{id:"summaries-labeling-neural-representations-with-inverse-recognition",title:"Labeling Neural Representations with Inverse Recognition",description:"summary of Labeling Neural Representations with Inverse Recognition",section:"Summaries",handler:()=>{window.location.href="/summaries/Labeling_Neural_Representations_with_Inverse_Recognition/"}},{id:"summaries-learning-decision-trees-from-random-examples",title:"Learning decision trees from random examples",description:"Decision tree learning By Finding Consistent Decision Trees",section:"Summaries",handler:()=>{window.location.href="/summaries/Learning_decision_trees_from_random_examples/"}},{id:"summaries-leveraged-volume-sampling-for-linear-regression",title:"Leveraged volume sampling for linear regression",description:"Active Learning in linear regression with multiplicative error rate bounds",section:"Summaries",handler:()=>{window.location.href="/summaries/Leveraged_volume_sampling_for_linear_regression/"}},{id:"summaries-physics-of-language-models",title:"Physics of Language Models",description:"Understanding LLMs by training smaller LMs in controlled environment",section:"Summaries",handler:()=>{window.location.href="/summaries/Physics_of_LM/"}},{id:"summaries-progress-measures-for-grokking-via-mechanistic-interpretability",title:"Progress measures for grokking via mechanistic interpretability",description:"summary of Progress measures for grokking via mechanistic interpretability",section:"Summaries",handler:()=>{window.location.href="/summaries/Progress_measures_for_grokking_via_mechanistic_interpretability/"}},{id:"summaries-properly-learning-decision-trees-in-almost-polynomial-time",title:"Properly learning decision trees in almost polynomial time",description:"learning a decision tree for unifrom random data distribution in O(s ^ log(log(s)))",section:"Summaries",handler:()=>{window.location.href="/summaries/Properly_learning-_decision_trees_in_almost_polynomial_time/"}},{id:"summaries-scaling-monosemanticity-extracting-interpretable-features-from-claude-3-sonnet",title:"Scaling Monosemanticity Extracting Interpretable Features from Claude 3 Sonnet",description:"Scale SAE to Claude 3 Sonnet",section:"Summaries",handler:()=>{window.location.href="/summaries/Scaling_Monosemanticity_Extracting_Interpretable_Features_from_Claude_3_Sonnet/"}},{id:"summaries-top-down-induction-of-decision-trees-rigorous-guarantees-and-inherent-limitations",title:"Top-down induction of decision trees- rigorous guarantees and inherent limitations",description:"greedily learn a decision tree based on the most inflouential variables in all leaves.",section:"Summaries",handler:()=>{window.location.href="/summaries/Top_down_induction_of_decision_trees_rigorous_guarantees_and_inherent_limitations/"}},{id:"summaries-towards-monosemanticity-decomposing-language-models-with-dictionary-learning",title:"Towards Monosemanticity Decomposing Language Models With Dictionary Learning",description:"How SAE works",section:"Summaries",handler:()=>{window.location.href="/summaries/Towards_Monosemanticity_Decomposing_Language_Models_With_Dictionary_Learning/"}},{id:"summaries-what-do-we-learn-from-inverting-clip-models",title:"What do we learn from inverting CLIP models?",description:"summary of What do we learn from inverting CLIP models?",section:"Summaries",handler:()=>{window.location.href="/summaries/What_do_we_learn_from_inverting_CLIP_models/"}},{id:"summaries-zoom-in-an-introduction-to-circuits",title:"Zoom In An Introduction to Circuits",description:"Investigate Vision Circuits by Studying the Connections between Neurons",section:"Summaries",handler:()=>{window.location.href="/summaries/Zoom_In_An_Introduction_to_Circuits/"}},{id:"summaries-active-learning-survey",title:"Active Learning Survey",description:"Active Learning for Agnostic classification",section:"Summaries",handler:()=>{window.location.href="/summaries/active-survey/"}},{id:"summaries-the-true-sample-complexity-of-active-learning",title:"The True Sample Complexity of Active Learning",description:"A different definition of active learning label complexity",section:"Summaries",handler:()=>{window.location.href="/summaries/true-active/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%72%73%68%69%61.%73%6F%6C%74%61%6E%69%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=2H6Wl4MAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ckodser","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>