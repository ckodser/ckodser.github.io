<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer | Arshia Soltani Moakhar </title> <meta name="author" content="Arshia Soltani Moakhar"> <meta name="description" content="This work investigates the internal mechanics of the Huginn depth-recurrent Transformer on arithmetic tasks to find evidence for latent chain-of-thought reasoning."> <meta name="keywords" content="academic-website, research, Arshia Soltani Moakhar, ckodser, adversarial training, OOD detection, Interpretability"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?16404ec2cd2689e8d0f38f73fe0d38f9"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon3.ico?ed0664bb8b662bf21b84ddd264d8c2a9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ckodser.github.io/summaries/Latent_Chain_of_Thought_Decoding_the_Depth_Recurrent_Transformer/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class=" sticky-bottom-footer collection-summaries"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">Arshia</span> Soltani Moakhar </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/summaries/">Summaries <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer</h1> <p class="post-description">This work investigates the internal mechanics of the Huginn depth-recurrent Transformer on arithmetic tasks to find evidence for latent chain-of-thought reasoning.</p> <a href="https://arxiv.org/pdf/2507.02199" style="display: inline-block; padding: 8px 0; font-weight: 500; text-decoration: none; border-bottom: 2px solid transparent; transition: border-color 0.3s ease; background: linear-gradient(135deg, #5e72e4, #ff7eb3); -webkit-background-clip: text; background-clip: text; -webkit-text-fill-color: transparent;" onmouseover="this.style.borderBottom='2px solid #ff7eb3'" onmouseout="this.style.borderBottom='2px solid transparent'" target="_blank" rel="noopener noreferrer">Read Paper →</a> </header> <article> <h3 id="method">Method</h3> <p>The internal states of the Huginn 3.5B model are analyzed using an unrolled view of its architecture. Hidden states are decoded using two methods: logit lens and coda lens.</p> <h4 id="unrolled-view-of-huginn-architecture">Unrolled View of Huginn Architecture</h4> <p>The model’s architecture consists of 2 Prelude blocks (\(\{P_1, P_2\}\)), 4 Recurrent Blocks (\(\{R_1, R_2, R_3, R_4\}\)), and 2 Coda blocks (\(\{C_1, C_2\}\)). At inference, input embeddings pass through the Prelude blocks, then undergo \(r\) recurrent passes through the Recurrent blocks, and finally pass through the Coda blocks for prediction. This process can be viewed as a single forward pass through \(2 + 4r + 2\) unrolled blocks.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Latent_Chain_of_Thought_Decoding_the_Depth_Recurrent_Transformer/image1.png" sizes="95vw"></source> <img src="/assets/img/Latent_Chain_of_Thought_Decoding_the_Depth_Recurrent_Transformer/image1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The hidden state \(s_i\) produced by each block \(i\) in this unrolled view is defined as:</p> \[s_i = \begin{cases} e &amp; i=0 \\ P_i(s_{i-1}) &amp; 1 \le i \le 2 \\ R_1(s_2, n), \quad n \sim \mathcal{N}(0, \sigma^2I_{L \cdot d}) &amp; i=3 \\ R_{(i-3) \pmod 4 + 1}(s_{i-1}) &amp; 4 \le i \le 2+4r, \ i \not\equiv 3 \pmod 4 \\ R_1(s_2, s_{i-1}) &amp; 4 \le i \le 2+4r, \ i \equiv 3 \pmod 4 \\ C_{i-(2+4r)}(s_{i-1}) &amp; 2+4r+1 \le i \le 2+4r+2 \end{cases}\] <p>In the third line, a random vector initializes the recurrence. This unrolled perspective provides access to \(2 + 4r + 2\) intermediate hidden states for analysis.</p> <h4 id="decoding-hidden-states-by-logit-lens-and-coda-lens">Decoding Hidden States by Logit Lens and Coda Lens</h4> <p>Two methods are used to decode hidden states \(s_i\) into logits over the vocabulary. Analysis focuses on the logits corresponding to the last token position, \(z_i[-1]\).</p> <p><strong>Logit Lens</strong>: Each hidden state \(s_i\) is normalized and then projected into the vocabulary space using the unembedding matrix \(W_U \in \mathbb{R}^{d \times \lvert V \rvert}\).</p> \[z_i = \text{RMSNorm}(s_i) W_U\] <p><strong>Coda Lens</strong>: Hidden states are decoded using the model’s Coda module, \(C = \{C_1, C_2\}\), which consists of two transformer blocks. Normalization is applied before and after the Coda module.</p> \[z_i = \text{RMSNorm}(C(\text{RMSNorm}(s_i))) W_U\] <h3 id="experiments">Experiments</h3> <p>The model’s behavior is examined on arithmetic tasks where explicit chain-of-thought (CoT) is suppressed.</p> <h4 id="experimental-setup">Experimental Setup</h4> <p><strong>Datasets</strong>: The primary dataset is the one-digit composite arithmetic task from Brown et al. (2020), which contains 2k questions (e.g., “What is (9 + 8) * 2? Answer: 34”). The model’s performance is also evaluated on the GSM8K math word problem dataset.</p> <p><strong>Suppress Explicit CoT</strong>: To encourage latent reasoning, explicit CoT is suppressed by instructing the model to output only the final answer. This is done via a system message (“You are a concise and helpful assistant. Always return only the final answer straightway.”) and four in-context examples showing the direct question-to-answer format. For all probing experiments, the number of recurrent steps is set to 16, resulting in a total of 68 unrolled blocks.</p> <h4 id="discontinuities-in-hidden-state-interpretability">Discontinuities in Hidden State Interpretability</h4> <p>The interpretability of hidden states across the unrolled layers is examined by decoding them with both logit lens and coda lens. The average rank of the final correct token is tracked across all 68 blocks for 100 arithmetic questions.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Latent_Chain_of_Thought_Decoding_the_Depth_Recurrent_Transformer/image2.png" sizes="95vw"></source> <img src="/assets/img/Latent_Chain_of_Thought_Decoding_the_Depth_Recurrent_Transformer/image2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The rank trajectories show periodic oscillations. Using the logit lens, the token rank spikes upwards (becomes worse) at each \(R_4\) block. Conversely, using the coda lens, the rank spikes downwards (becomes better) at each \(R_4\) block.</p> <p><strong>I don’t know why they use logit lens. It doesn’t make that much sense here. The Coda lens method in my oppinion is much better and should have been the sole interpretability method used</strong>.</p> <h4 id="tracing-final-and-intermediate-tokens">Tracing Final and Intermediate Tokens</h4> <p>To search for evidence of multi-step reasoning, the rank trajectories of intermediate and final result tokens are traced across recurrent steps. The analysis is performed on a filtered subset of 67 correctly answered questions where the intermediate and final results are distinct single-digit tokens. To reduce the effect of oscillations, the analysis focuses on outputs from block \(R_3\) (for logit lens) and \(R_4\) (for coda lens).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Latent_Chain_of_Thought_Decoding_the_Depth_Recurrent_Transformer/image4.png" sizes="95vw"></source> <img src="/assets/img/Latent_Chain_of_Thought_Decoding_the_Depth_Recurrent_Transformer/image4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The results show that the ranks of both final and intermediate tokens decrease early in the recurrence. The final token consistently maintains a lower rank than the intermediate token. There is no clear delay between the drop in rank for the intermediate token and the final token, which would be expected in a sequential reasoning process. A rank reversal is observed around step 6, where the intermediate token’s rank briefly drops below the final token’s rank in some examples.</p> <h4 id="scaling-recurrent-steps">Scaling Recurrent Steps</h4> <p>The model’s performance on the GSM8K dataset is evaluated as the number of recurrent steps is varied, with explicit CoT suppressed.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Latent_Chain_of_Thought_Decoding_the_Depth_Recurrent_Transformer/image5.png" sizes="95vw"></source> <img src="/assets/img/Latent_Chain_of_Thought_Decoding_the_Depth_Recurrent_Transformer/image5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>As shown in the results, increasing the number of recurrent steps from 4 to 32 yields a modest increase in accuracy (from 3.11% to 4.93%), after which performance plateaus. In contrast, the same model with explicit CoT prompting achieves a much higher accuracy of 24.87%/38.13%. This suggests that increasing computational depth through recurrence alone is insufficient to match the performance of externalized reasoning steps.</p> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ckodser/ckodser.github.io","data-repo-id":"R_kgDOHfm8sw","data-category":"General","data-category-id":"DIC_kwDOHfm8s84ClE4O","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"light","data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © 2026 Arshia Soltani Moakhar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/summary_image_expand.js?de7a891220c8fc9ce98add9ca26b742e"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?f8abf2f636f242d077f24149a0a56c96"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"My Research Projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-summaries",title:"Summaries",description:"Paper summaries",section:"Navigation",handler:()=>{window.location.href="/summaries/"}},{id:"projects-data-leakage-of-lora-in-federated-training",title:"Data Leakage of LoRA in federated training",description:"This article highlights the potential risks of reconstructing private data from the gradients shared in Federated Learning, especially when using the LoRA finetuning technique.",section:"Projects",handler:()=>{window.location.href="/projects/Attention_is_all_you_need_to_attack/"}},{id:"projects-basedon",title:"BasedOn",description:"Using Learnable If Statements for Interpretability",section:"Projects",handler:()=>{window.location.href="/projects/BasedOn/"}},{id:"projects-sparsity-for-interpretability",title:"sparsity for interpretability",description:"Leveraging sample sparsity to improve interpretability of neural networks",section:"Projects",handler:()=>{window.location.href="/projects/SPADE/"}},{id:"projects-certified-robust-neural-network",title:"Certified Robust Neural Network",description:"Certify Robustness using median neurons",section:"Projects",handler:()=>{window.location.href="/projects/sparse_L_inf_network/"}},{id:"summaries-a-mathematical-framework-for-transformer-circuits",title:"A Mathematical Framework for Transformer Circuits",description:"In Transformers residual stream is the main object and layers read and write from/to it.",section:"Summaries",handler:()=>{window.location.href="/summaries/A_Mathematical_Framework_for_Transformer_Circuits/"}},{id:"summaries-an-overview-of-early-vision-in-inceptionv1",title:"An Overview of Early Vision in InceptionV1",description:"inceptionV1 feature maps of different layers",section:"Summaries",handler:()=>{window.location.href="/summaries/An_Overview_of_Early_Vision_in_InceptionV1/"}},{id:"summaries-clip-dissect-automatic-description-of-neuron-representations",title:"CLIP-Dissect Automatic Description of Neuron Representations",description:"Find concepts that activates a neuron using a image dataset",section:"Summaries",handler:()=>{window.location.href="/summaries/CLIP-Dissect_Automatic_Description_of_Neuron_Representations_in_Deep_Vision_Networks/"}},{id:"summaries-can-large-language-models-explain-their-internal-mechanisms",title:"Can Large Language Models Explain Their Internal Mechanisms?",description:"summary of Can Large Language Models Explain Their Internal Mechanisms?",section:"Summaries",handler:()=>{window.location.href="/summaries/Can_Large_Language_Models_Explain_Their_Internal_Mechanisms/"}},{id:"summaries-chain-of-images-for-intuitively-reasoning",title:"Chain of Images for Intuitively Reasoning",description:"This paper proposes a Chain-of-Images (CoI) method for multimodal models to solve reasoning problems by generating a series of images as intermediate representations, using a Symbolic Multimodal Large Language Model (SyMLLM).",section:"Summaries",handler:()=>{window.location.href="/summaries/Chain_of_Images_for_Intuitively_Reasoning/"}},{id:"summaries-dash-detection-and-assessment-of-systematic-hallucinations-of-vlms",title:"DASH Detection and Assessment of Systematic Hallucinations of VLMs",description:"Make a dataset that VLMs hallucinate and wrongly think things exist in images",section:"Summaries",handler:()=>{window.location.href="/summaries/DASH_Detection_and_Assessment_of_Systematic_Hallucinations_of_VLMs/"}},{id:"summaries-deep-learning-is-not-so-mysterious-or-different",title:"Deep Learning is Not So Mysterious or Different",description:"??",section:"Summaries",handler:()=>{window.location.href="/summaries/Deep_Learning_is_Not_So_Mysterious_or_Different/"}},{id:"summaries-dream-7b-diffusion-large-language-models",title:"Dream 7B Diffusion Large Language Models",description:"Dream 7B is a 7-billion parameter diffusion language model that refines text in parallel, initialized from an autoregressive model and trained using context-adaptive noise scheduling.",section:"Summaries",handler:()=>{window.location.href="/summaries/Dream_7B_Diffusion_Large_Language_Models/"}},{id:"summaries-emergent-world-representations-exploring-a-sequence-model-trained-on-a-synthetic-task",title:"Emergent World Representations Exploring a Sequence Model Trained on a Synthetic Task",description:"summary of Emergent World Representations  Exploring a Sequence Model Trained on a Synthetic Task",section:"Summaries",handler:()=>{window.location.href="/summaries/Emergent_World_Representations_Exploring_a_Sequence_Model_Trained_on_a_Synthetic_Task/"}},{id:"summaries-every-decision-tree-has-an-influential-variable",title:"Every decision tree has an influential variable",description:"title is self-explanatory",section:"Summaries",handler:()=>{window.location.href="/summaries/Every_decision_tree_has_an_influential_variable/"}},{id:"summaries-imagine-while-reasoning-in-space-multimodal-visualization-of-thought",title:"Imagine while Reasoning in Space Multimodal Visualization-of-Thought",description:"Multimodal Visualization-of-Thought (MVoT) is proposed to enable Multimodal Large Language Models (MLLMs) to generate interleaved verbal and visual reasoning traces for spatial reasoning tasks.",section:"Summaries",handler:()=>{window.location.href="/summaries/Imagine_while_Reasoning_in_Space_Multimodal_Visualization_of_Thought/"}},{id:"summaries-interpretability-beyond-feature-attribution-quantitative-testing-with-concept-activation-vectors-tcav",title:"Interpretability Beyond Feature Attribution Quantitative Testing with Concept Activation Vectors (TCAV)",description:"summary of Interpretability Beyond Feature Attribution  Quantitative Testing with Concept Activation Vectors (TCAV)",section:"Summaries",handler:()=>{window.location.href="/summaries/Interpretability_Beyond_Feature_Attribution_Quantitative_Testing_with_Concept_Activation_Vectors_(TCAV)/"}},{id:"summaries-llm-latent-reasoning-as-chain-of-superposition",title:"LLM Latent Reasoning as Chain of Superposition",description:"Train an encoder that summarizes reasoning chunks. Then train a latent reasoning model on the summaries it produces from some CoT data.",section:"Summaries",handler:()=>{window.location.href="/summaries/LLM_Latent_Reasoning_as_Chain_of_Superposition/"}},{id:"summaries-llms-are-single-threaded-reasoners-demystifying-the-working-mechanism-of-soft-thinking",title:"LLMs are Single-threaded Reasoners, Demystifying the Working Mechanism of Soft Thinking",description:"Vanilla Soft Thinking pushes the model to the greedy token sampling internally. They showed that the model usually continues to work only with the most probable next token. To mitigate this issue they suggest adding noise to logits and get better performance.",section:"Summaries",handler:()=>{window.location.href="/summaries/LLMs_are_Single-threaded_Reasoners_Demystifying_the_Working_Mechanism_of_Soft_Thinking/"}},{id:"summaries-labeling-neural-representations-with-inverse-recognition",title:"Labeling Neural Representations with Inverse Recognition",description:"summary of Labeling Neural Representations with Inverse Recognition",section:"Summaries",handler:()=>{window.location.href="/summaries/Labeling_Neural_Representations_with_Inverse_Recognition/"}},{id:"summaries-latent-chain-of-thought-decoding-the-depth-recurrent-transformer",title:"Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer",description:"This work investigates the internal mechanics of the Huginn depth-recurrent Transformer on arithmetic tasks to find evidence for latent chain-of-thought reasoning.",section:"Summaries",handler:()=>{window.location.href="/summaries/Latent_Chain_of_Thought_Decoding_the_Depth_Recurrent_Transformer/"}},{id:"summaries-latent-sketchpad-sketching-visual-thoughts-to-elicit-multimodal-reasoning-in-mllms",title:"Latent Sketchpad Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs",description:"A framework that equips Multimodal Large Language Models with an internal visual scratchpad to generate visual latents during autoregressive reasoning, which can be translated into interpretable sketches.",section:"Summaries",handler:()=>{window.location.href="/summaries/Latent_Sketchpad_Sketching_Visual_Thoughts_to_Elicit_Multimodal_Reasoning_in_MLLMs/"}},{id:"summaries-learning-decision-trees-from-random-examples",title:"Learning decision trees from random examples",description:"Decision tree learning By Finding Consistent Decision Trees",section:"Summaries",handler:()=>{window.location.href="/summaries/Learning_decision_trees_from_random_examples/"}},{id:"summaries-leveraged-volume-sampling-for-linear-regression",title:"Leveraged volume sampling for linear regression",description:"Active Learning in linear regression with multiplicative error rate bounds",section:"Summaries",handler:()=>{window.location.href="/summaries/Leveraged_volume_sampling_for_linear_regression/"}},{id:"summaries-multiplex-thinking-reasoning-via-token-wise-branch-and-merge",title:"Multiplex Thinking Reasoning via Token wise Branch and Merge",description:"Make soft-thinking a bit random. Then train with GRPO.",section:"Summaries",handler:()=>{window.location.href="/summaries/Multiplex_Thinking_Reasoning_via_Token_wise_Branch_and_Merge/"}},{id:"summaries-physics-of-language-models",title:"Physics of Language Models",description:"Understanding LLMs by training smaller LMs in controlled environment",section:"Summaries",handler:()=>{window.location.href="/summaries/Physics_of_LM/"}},{id:"summaries-progress-measures-for-grokking-via-mechanistic-interpretability",title:"Progress measures for grokking via mechanistic interpretability",description:"summary of Progress measures for grokking via mechanistic interpretability",section:"Summaries",handler:()=>{window.location.href="/summaries/Progress_measures_for_grokking_via_mechanistic_interpretability/"}},{id:"summaries-properly-learning-decision-trees-in-almost-polynomial-time",title:"Properly learning decision trees in almost polynomial time",description:"learning a decision tree for unifrom random data distribution in O(s ^ log(log(s)))",section:"Summaries",handler:()=>{window.location.href="/summaries/Properly_learning-_decision_trees_in_almost_polynomial_time/"}},{id:"summaries-reasoning-within-the-mind-dynamic-multimodal-interleaving-in-latent-space",title:"Reasoning Within the Mind Dynamic Multimodal Interleaving in Latent Space",description:"Training-free latent reasoning. Optimize latent reasoning tokens to maximize model confidence, which correlates with accuracy.",section:"Summaries",handler:()=>{window.location.href="/summaries/Reasoning_Within_the_Mind_Dynamic_Multimodal_Interleaving_in_Latent_Space/"}},{id:"summaries-scaling-latent-reasoning-via-looped-language-models",title:"Scaling Latent Reasoning via Looped Language Models",description:"Training a recurrent reasoning model. Looping the same models over and over again.",section:"Summaries",handler:()=>{window.location.href="/summaries/Scaling_Latent_Reasoning_via_Looped_Language_Models/"}},{id:"summaries-scaling-monosemanticity-extracting-interpretable-features-from-claude-3-sonnet",title:"Scaling Monosemanticity Extracting Interpretable Features from Claude 3 Sonnet",description:"Scale SAE to Claude 3 Sonnet",section:"Summaries",handler:()=>{window.location.href="/summaries/Scaling_Monosemanticity_Extracting_Interpretable_Features_from_Claude_3_Sonnet/"}},{id:"summaries-soft-tokens-hard-truths",title:"Soft Tokens, Hard Truths",description:"They add Gaussian noise to the soft-thinking embeddings, then train with RL using RLOO.",section:"Summaries",handler:()=>{window.location.href="/summaries/Soft_Tokens_Hard_Truths/"}},{id:"summaries-top-down-induction-of-decision-trees-rigorous-guarantees-and-inherent-limitations",title:"Top-down induction of decision trees- rigorous guarantees and inherent limitations",description:"greedily learn a decision tree based on the most inflouential variables in all leaves.",section:"Summaries",handler:()=>{window.location.href="/summaries/Top_down_induction_of_decision_trees_rigorous_guarantees_and_inherent_limitations/"}},{id:"summaries-towards-monosemanticity-decomposing-language-models-with-dictionary-learning",title:"Towards Monosemanticity Decomposing Language Models With Dictionary Learning",description:"How SAE works",section:"Summaries",handler:()=>{window.location.href="/summaries/Towards_Monosemanticity_Decomposing_Language_Models_With_Dictionary_Learning/"}},{id:"summaries-what-do-we-learn-from-inverting-clip-models",title:"What do we learn from inverting CLIP models?",description:"summary of What do we learn from inverting CLIP models?",section:"Summaries",handler:()=>{window.location.href="/summaries/What_do_we_learn_from_inverting_CLIP_models/"}},{id:"summaries-zebra-cot-a-dataset-for-interleaved-vision-language-reasoning",title:"Zebra-CoT A Dataset for Interleaved Vision Language Reasoning",description:"Zebra-CoT is a large-scale dataset with 182,384 interleaved text-image reasoning traces across 18 domains for training multimodal models.",section:"Summaries",handler:()=>{window.location.href="/summaries/Zebra_CoT_A_Dataset_for_Interleaved_Vision_Language_Reasoning/"}},{id:"summaries-zoom-in-an-introduction-to-circuits",title:"Zoom In An Introduction to Circuits",description:"Investigate Vision Circuits by Studying the Connections between Neurons",section:"Summaries",handler:()=>{window.location.href="/summaries/Zoom_In_An_Introduction_to_Circuits/"}},{id:"summaries-active-learning-survey",title:"Active Learning Survey",description:"Active Learning for Agnostic classification",section:"Summaries",handler:()=>{window.location.href="/summaries/active-survey/"}},{id:"summaries-the-true-sample-complexity-of-active-learning",title:"The True Sample Complexity of Active Learning",description:"A different definition of active learning label complexity",section:"Summaries",handler:()=>{window.location.href="/summaries/true-active/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%72%73%68%69%61.%73%6F%6C%74%61%6E%69%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=2H6Wl4MAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ckodser","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>