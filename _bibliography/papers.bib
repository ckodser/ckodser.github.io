---
---

@article{NeurIPS,
  abbr={NeurIPS},
  title={Your Out-of-Distribution Detection Method is not Robust! <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/1f6591cc41be737e9ba4cc487ac8082d-Abstract-Conference.html">(NeurIPS)</a>},
  author={Mohammad Azizmalayeri and Arshia Soltani Moakhar and Arman Zarei and Reihaneh Zohrabi and Mohammad Taghi Manzuri and Mohammad Hossein Rohban},
  abstract={
     Out-of-distribution (OOD) detection has recently gained substantial attention due to the importance of identifying out-of-domain samples in reliability and safety. Although OOD detection methods have advanced by a great deal, they are still susceptible to adversarial examples, which is a violation of their purpose. To mitigate this issue, several defenses have recently been proposed. Nevertheless, these efforts remained ineffective, as their evaluations are based on either small perturbation sizes, or weak attacks. In this work, we re-examine these defenses against an end-to-end PGD attack on in/out data with larger perturbation sizes, e.g. up to commonly used  for the CIFAR-10 dataset. Surprisingly, almost all of these defenses perform worse than a random detection under the adversarial setting. Next, we aim to provide a robust OOD detection method. In an ideal defense, the training  should expose the model to almost {\it all} possible adversarial perturbations, which can be achieved through adversarial training. That is, such training perturbations should based on both in- and out-of-distribution samples. Therefore, unlike OOD detection in the standard setting, access to OOD, as well as in-distribution, samples sounds necessary in the adversarial training setup. These tips lead us to adopt generative OOD detection methods, such as OpenGAN, as a baseline. We subsequently propose the Adversarially Trained Discriminator (ATD), which utilizes a pre-trained robust model to extract robust features, and a generator model to create OOD samples. We noted that, for the sake of training stability, in the adversarial training of the discriminator, one should attack real in-distribution as well as real outliers, but not  generated outliers. Using ATD with CIFAR-10 and CIFAR-100 as the in-distribution data, we could significantly outperform all previous methods in the robust AUROC while maintaining high standard AUROC and classification accuracy.
  },
  journal={NeurIPS},
  year={2022},
  publisher=aps,
  selected={true}
}

@article{moakhar2023spade,
    abbr={ICML},
  title={SPADE: Sparsity-Guided Debugging for Deep Neural Networks. <a href="https://icml.cc/virtual/2024/poster/33092">(ICML)</a>},
  author={Arshia Soltani Moakhar* and Eugenia Iofinova* and Elias Frantar and Dan Alistarh},
  journal={ICML, NeurIPS ATTRIB workshop},
  year={2024},
  abstract={Interpretability, broadly defined as mechanisms for understanding why and how machine learning models reach their decisions, is one of the key open goals at the intersection of deep learning theory and practice. Towards this goal, multiple tools have been proposed to aid a human examiner in reasoning about a network's behavior in general or on a set of instances. However, the outputs of these tools-such as input saliency maps or neuron visualizations-are frequently difficult for a human to interpret, or even misleading, due, in particular, to the fact that neurons can be multifaceted, i.e., a single neuron can be associated with multiple distinct feature combinations. In this paper, we present a new general approach to address this problem, called SPADE, which, given a trained model and a target sample, uses sample-targeted pruning to provide a "trace" of the network's execution on the sample, reducing the network to the connections that are most relevant to the specific prediction. We demonstrate that preprocessing with SPADE significantly increases both the accuracy of image saliency maps across several interpretability methods and the usefulness of neuron visualizations, aiding humans in reasoning about network behavior. Our findings show that sample-specific pruning of connections can disentangle multifaceted neurons, leading to consistently improved interpretability.
  },
  selected={true}
}

@article{moakhar2023seeking,
      title={Seeking Next Layer Neurons' Attention for Error-Backpropagation-Like Training in a Multi-Agent Network Framework. <a href="https://arxiv.org/abs/2310.09952"> (arXiv)</a>},
      author={Arshia Soltani Moakhar and Mohammad Azizmalayeri and Hossein Mirzaei and Mohammad Taghi Manzuri and Mohammad Hossein Rohban},
      journal={arXiv},
      year={2023},
      abstract={Despite considerable theoretical progress in the training of neural networks viewed as a multi-agent system of neurons, particularly concerning biological plausibility and decentralized training, their applicability to real-world problems remains limited due to scalability issues. In contrast, error-backpropagation has demonstrated its effectiveness for training deep networks in practice. In this study, we propose a local objective for neurons that, when pursued by neurons individually, align them to exhibit similarities to error-backpropagation in terms of efficiency and scalability during training. For this purpose, we examine a neural network comprising decentralized, self-interested neurons seeking to maximize their local objective -- attention from subsequent layer neurons -- and identify the optimal strategy for neurons. We also analyze the relationship between this strategy and backpropagation, establishing conditions under which the derived strategy is equivalent to error-backpropagation. Lastly, we demonstrate the learning capacity of these multi-agent neural networks through experiments on three datasets and showcase their superior performance relative to error-backpropagation in a catastrophic forgetting benchmark.},
  selected={false}
}

@article{rodeo,
      abbr={ICML},
      title={RODEO: Robust Out-of-Distribution Detection Via Exposing Adaptive Outliers. <a href="https://icml.cc/virtual/2024/poster/32673"> (ICML)</a>},
      author={Hossein Mirzaei and Mohammad Jafari and Hamid Reza Dehbashi and Ali Ansari and Sepehr Ghobadi and Masoud Hadi and Arshia Soltani Moakhar and Mohammad Azizmalayeri and Mahdieh Soleymani Baghshah and Mohammad Hossein Rohban},
      journal={ICML},
      year={2024},
      abstract={Detecting out-of-distribution (OOD) input samples at the time of inference is a key element in the trustworthy deployment of intelligent models. While there has been tremendous improvement in various variants of OOD detection in recent years, detection performance under adversarial settings lags far behind the performance in the standard setting. In order to bridge this gap, we introduce RODEO in this paper, a data-centric approach that generates effective outliers for robust OOD detection. More specifically, we first show that targeting the classification of adversarially perturbed in- and out-of-distribution samples through outlier exposure (OE) could be an effective strategy for the mentioned purpose, as long as the training outliers meet certain quality standards. We hypothesize that the outliers in the OE should possess several characteristics simultaneously to be effective in the adversarial training: diversity, and both conceptual differentiability and analogy to the inliers. These aspects seem to play a more critical role in the adversarial setup compared to the standard training. Next, we propose an adaptive OE method to generate near and diverse outliers by incorporating both text and image domain information. This process helps satisfy the mentioned criteria for the generated outliers and significantly enhances the performance of the OE technique, particularly in adversarial settings. Our method demonstrates its effectiveness across various detection setups, such as novelty detection (ND), Open-Set Recognition (OSR), and out-of-distribution (OOD) detection. Furthermore, we conduct a comprehensive comparison of our approach with other OE techniques in adversarial settings to showcase its effectiveness.},
  selected={true}
}

@inproceedings{romanou2025include,
    abbr={ICLR Spotlight},
    title={{INCLUDE}: Evaluating Multilingual Language Understanding with Regional Knowledge <a href="https://arxiv.org/pdf/2411.19799"> (ICLR)</a>},
    author={Angelika Romanou and Negar Foroutan and Anna Sotnikova and Sree Harsha Nelaturu and Shivalika Singh and Rishabh Maheshwary and Micol Altomare and Zeming Chen and Mohamed A. Haggag and Snegha A and Alfonso Amayuelas and Azril Hafizi Amirudin and Danylo Boiko and Michael Chang and Jenny Chim and Gal Cohen and Aditya Kumar Dalmia and Abraham Diress and Sharad Duwal and Daniil Dzenhaliou and Daniel Fernando Erazo Florez and Fabian Farestam and Joseph Marvin Imperial and Shayekh Bin Islam and Perttu Isotalo and Maral Jabbarishiviari and B{\"o}rje F. Karlsson and Eldar Khalilov and Christopher Klamm and Fajri Koto and Dominik Krzemi{\'n}ski and Gabriel Adriano de Melo and Syrielle Montariol and Yiyang Nan and Joel Niklaus and Jekaterina Novikova and Johan Samir Obando Ceron and Debjit Paul and Esther Ploeger and Jebish Purbey and Swati Rajwal and Selvan Sunitha Ravi and Sara Rydell and Roshan Santhosh and Drishti Sharma and Marjana Prifti Skenduli and Arshia Soltani Moakhar and Bardia soltani moakhar and Ayush Kumar Tarun and Azmine Toushik Wasi and Thenuka Ovin Weerasinghe and Serhan Yilmaz and Mike Zhang and Imanol Schlag and Marzieh Fadaee and Sara Hooker and Antoine Bosselut},
    booktitle={ICLR},
    year={2025},
    url={https://openreview.net/forum?id=k3gCieTXeY},
    abstract={The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.},
    selected={true}
}

@article{jabbarishiviari2024software,
    abbr={ICLR},
    title={Software 1.0 Strengths for Interpretability and Data Efficiency <a href="https://openreview.net/forum?id=gyl8r8ANcd"> (ICLR TinyPapers)</a>},
    author={Maral Jabbarishiviari and Arshia Soltani Moakhar},
    booktitle={The Second Tiny Papers Track at ICLR 2024},
    year={2024},
    url={https://openreview.net/forum?id=gyl8r8ANcd},
    abstract={Machine learning has demonstrated remarkable capabilities across various tasks, yet it confronts significant challenges such as limited interpretability, reliance on extensive data, and difficulties in incorporating human intuition. In contrast, traditional software development avoids these pitfalls, offering full interpretability, less data dependency, and easy integration of intuitive decision-making. To have the strengths of both approaches, we introduce the BasedOn library. This tool focuses on code written by programmers while providing very simple interfaces to let programmers use machine learning. The BasedOn library, leveraging policy gradient methods, offers "learnable" if statements.},
    selected={true}
}

@misc{salazar2025kaleidoscopeinlanguageexamsmassively,
      abbr={ICLR},
      title={Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation <a href="https://openreview.net/forum?id=zCYXhSy9UH"> (ICLR)</a>},
      author={Israfel Salazar* and Manuel Fernández Burda* and Shayekh Bin Islam* and Arshia Soltani Moakhar* and Shivalika Singh* and Fabian Farestam* and Angelika Romanou* and Danylo Boiko and Dipika Khullar and Mike Zhang and Dominik Krzemiński and Jekaterina Novikova and Luísa Shimabucoro and Joseph Marvin Imperial and Rishabh Maheshwary and Sharad Duwal and Alfonso Amayuelas and Swati Rajwal and Jebish Purbey and Ahmed Ruby and Nicholas Popovič and Marek Suppa and Azmine Toushik Wasi and Ram Mohan Rao Kadiyala and Olga Tsymboi and Maksim Kostritsya and Bardia Soltani Moakhar and Gabriel da Costa Merlin and Otávio Ferracioli Coletti and Maral Jabbari Shiviari and MohammadAmin farahani fard and Silvia Fernandez and María Grandury and Dmitry Abulkhanov and Drishti Sharma and Andre Guarnier De Mitri and Leticia Bossatto Marchezi and Johan Obando-Ceron and Nazar Kohut and Beyza Ermis and Desmond Elliott and Enzo Ferrante and Sara Hooker and Marzieh Fadaee},
      year={2025},
      eprint={2504.07072},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.07072},
      abstract={The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam benchmark to date for the multilingual evaluation of vision-language models. Kaleidoscope is a large-scale, in-language multimodal benchmark designed to evaluate VLMs across diverse languages and visual inputs. Kaleidoscope covers 18 languages and 14 different subjects, amounting to a total of 20,911 multiple-choice questions. Built through an open science collaboration with a diverse group of researchers worldwide, Kaleidoscope ensures linguistic and cultural authenticity. We evaluate top-performing multilingual vision-language models and find that they perform poorly on low-resource languages and in complex multimodal scenarios. Our results highlight the need for progress on culturally inclusive multimodal evaluation frameworks.},
      selected={true}
}

@misc{jafariraviz2026failingexplorelanguagemodels,
      title={Failing to Explore: Language Models on Interactive Tasks <a href="https://arxiv.org/abs/2601.22345"> (arXiv)</a>}, 
      author={Mahdi JafariRaviz* and Keivan Rezaei* and Arshia Soltani Moakhar* and Zahra Sodagar and Yize Cheng and Soheil Feizi},
      year={2026},
      eprint={2601.22345},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2601.22345}, 
      abstract={We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.},
      selected={true}
}

@article{moakhar2026active,
abbr={ICLR},
  title={Active Learning for Decision Trees with Provable Guarantees <a href="https://openreview.net/forum?id=NOkjJPJIit"> (ICLR)</a>},
  author={Moakhar, Arshia Soltani and Laoaron, Tanapoom and Ghahremani, Faraz and Banihashem, Kiarash and Hajiaghayi, MohammadTaghi},
  journal={ICLR},
  year={2026},
  abstract={This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees—a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity: (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+\epsilon)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm’s dependence on the error tolerance $\epsilon$ is close to optimal.},
  selected={true}
}

@misc{cheng2026llmagentstemporallyblind,
      title={Your LLM Agents are Temporally Blind: The Misalignment Between Tool Use Decisions and Human Time Perception}, 
      author={Yize Cheng* and Arshia Soltani Moakhar* and Chenrui Fan* and Parsa Hosseini and Kazem Faghih and Zahra Sodagar and Wenxiao Wang and Soheil Feizi},
      year={2026},
      eprint={2510.23853},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.23853}, 
      abstract={Large language model (LLM) agents are increasingly used to interact with and execute tasks in dynamic environments. However, a critical yet overlooked limitation of these agents is that they, by default, assume a stationary context, failing to account for the real-world time elapsed between messages. We refer to this as "temporal blindness". This limitation hinders decisions about when to invoke tools, leading agents to either over-rely on stale context and skip needed tool calls, or under-rely on it and redundantly repeat tool calls. To study this challenge, we constructed TicToc, a diverse dataset of multi-turn user-agent message trajectories across 76 scenarios, spanning dynamic environments with high, medium, and low time sensitivity. We collected human preferences between "calling a tool" and "directly answering" on each sample, and evaluated how well LLM tool-calling decisions align with human preferences under varying amounts of elapsed time. Our analysis reveals that existing models display poor alignment with human temporal perception, with no model achieving a normalized alignment rate better than 65% when given time stamp information. We also show that naive, prompt-based alignment techniques have limited effectiveness for most models, but specific post-training alignment can be a viable way to align multi-turn LLM tool use with human temporal perception. Our data and findings provide a first step toward understanding and mitigating temporal blindness, offering insights to foster the development of more time-aware and human-aligned agents.},
      selected={true}
}

@misc{parast2026ghosthallucinationinducingimagegeneration,
      abbr={ICLR},
      title={GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs <a href="https://openreview.net/forum?id=f4TACE7HhU">(ICLR)</a>}, 
      author={Aryan Yazdan Parast and Parsa Hosseini and Hesam Asadollahzadeh and Arshia Soltani Moakhar and Basim Azam and Soheil Feizi and Naveed Akhtar},
      year={2026},
      eprint={2509.25178},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.25178}, 
}